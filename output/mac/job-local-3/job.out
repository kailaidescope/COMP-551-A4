Starting BERT epoch experiments script
Train method: head+1
Learning rate: 0.001 
Num epochs: 8 
Batch size: 32 
Weight decay: 1 
Warmup steps: 500
Loading model
Using online BERT model
Device: cpu
Name: bert.embeddings.word_embeddings.weight  - Size: torch.Size([30522, 768])  - Requires grad: False
Name: bert.embeddings.position_embeddings.weight  - Size: torch.Size([512, 768])  - Requires grad: False
Name: bert.embeddings.token_type_embeddings.weight  - Size: torch.Size([2, 768])  - Requires grad: False
Name: bert.embeddings.LayerNorm.weight  - Size: torch.Size([768])  - Requires grad: False
Name: bert.embeddings.LayerNorm.bias  - Size: torch.Size([768])  - Requires grad: False
Name: bert.encoder.layer.0.attention.self.query.weight  - Size: torch.Size([768, 768])  - Requires grad: False
Name: bert.encoder.layer.0.attention.self.query.bias  - Size: torch.Size([768])  - Requires grad: False
Name: bert.encoder.layer.0.attention.self.key.weight  - Size: torch.Size([768, 768])  - Requires grad: False
Name: bert.encoder.layer.0.attention.self.key.bias  - Size: torch.Size([768])  - Requires grad: False
Name: bert.encoder.layer.0.attention.self.value.weight  - Size: torch.Size([768, 768])  - Requires grad: False
Name: bert.encoder.layer.0.attention.self.value.bias  - Size: torch.Size([768])  - Requires grad: False
Name: bert.encoder.layer.0.attention.output.dense.weight  - Size: torch.Size([768, 768])  - Requires grad: False
Name: bert.encoder.layer.0.attention.output.dense.bias  - Size: torch.Size([768])  - Requires grad: False
Name: bert.encoder.layer.0.attention.output.LayerNorm.weight  - Size: torch.Size([768])  - Requires grad: False
Name: bert.encoder.layer.0.attention.output.LayerNorm.bias  - Size: torch.Size([768])  - Requires grad: False
Name: bert.encoder.layer.0.intermediate.dense.weight  - Size: torch.Size([3072, 768])  - Requires grad: False
Name: bert.encoder.layer.0.intermediate.dense.bias  - Size: torch.Size([3072])  - Requires grad: False
Name: bert.encoder.layer.0.output.dense.weight  - Size: torch.Size([768, 3072])  - Requires grad: False
Name: bert.encoder.layer.0.output.dense.bias  - Size: torch.Size([768])  - Requires grad: False
Name: bert.encoder.layer.0.output.LayerNorm.weight  - Size: torch.Size([768])  - Requires grad: False
Name: bert.encoder.layer.0.output.LayerNorm.bias  - Size: torch.Size([768])  - Requires grad: False
Name: bert.encoder.layer.1.attention.self.query.weight  - Size: torch.Size([768, 768])  - Requires grad: False
Name: bert.encoder.layer.1.attention.self.query.bias  - Size: torch.Size([768])  - Requires grad: False
Name: bert.encoder.layer.1.attention.self.key.weight  - Size: torch.Size([768, 768])  - Requires grad: False
Name: bert.encoder.layer.1.attention.self.key.bias  - Size: torch.Size([768])  - Requires grad: False
Name: bert.encoder.layer.1.attention.self.value.weight  - Size: torch.Size([768, 768])  - Requires grad: False
Name: bert.encoder.layer.1.attention.self.value.bias  - Size: torch.Size([768])  - Requires grad: False
Name: bert.encoder.layer.1.attention.output.dense.weight  - Size: torch.Size([768, 768])  - Requires grad: False
Name: bert.encoder.layer.1.attention.output.dense.bias  - Size: torch.Size([768])  - Requires grad: False
Name: bert.encoder.layer.1.attention.output.LayerNorm.weight  - Size: torch.Size([768])  - Requires grad: False
Name: bert.encoder.layer.1.attention.output.LayerNorm.bias  - Size: torch.Size([768])  - Requires grad: False
Name: bert.encoder.layer.1.intermediate.dense.weight  - Size: torch.Size([3072, 768])  - Requires grad: False
Name: bert.encoder.layer.1.intermediate.dense.bias  - Size: torch.Size([3072])  - Requires grad: False
Name: bert.encoder.layer.1.output.dense.weight  - Size: torch.Size([768, 3072])  - Requires grad: False
Name: bert.encoder.layer.1.output.dense.bias  - Size: torch.Size([768])  - Requires grad: False
Name: bert.encoder.layer.1.output.LayerNorm.weight  - Size: torch.Size([768])  - Requires grad: False
Name: bert.encoder.layer.1.output.LayerNorm.bias  - Size: torch.Size([768])  - Requires grad: False
Name: bert.encoder.layer.2.attention.self.query.weight  - Size: torch.Size([768, 768])  - Requires grad: False
Name: bert.encoder.layer.2.attention.self.query.bias  - Size: torch.Size([768])  - Requires grad: False
Name: bert.encoder.layer.2.attention.self.key.weight  - Size: torch.Size([768, 768])  - Requires grad: False
Name: bert.encoder.layer.2.attention.self.key.bias  - Size: torch.Size([768])  - Requires grad: False
Name: bert.encoder.layer.2.attention.self.value.weight  - Size: torch.Size([768, 768])  - Requires grad: False
Name: bert.encoder.layer.2.attention.self.value.bias  - Size: torch.Size([768])  - Requires grad: False
Name: bert.encoder.layer.2.attention.output.dense.weight  - Size: torch.Size([768, 768])  - Requires grad: False
Name: bert.encoder.layer.2.attention.output.dense.bias  - Size: torch.Size([768])  - Requires grad: False
Name: bert.encoder.layer.2.attention.output.LayerNorm.weight  - Size: torch.Size([768])  - Requires grad: False
Name: bert.encoder.layer.2.attention.output.LayerNorm.bias  - Size: torch.Size([768])  - Requires grad: False
Name: bert.encoder.layer.2.intermediate.dense.weight  - Size: torch.Size([3072, 768])  - Requires grad: False
Name: bert.encoder.layer.2.intermediate.dense.bias  - Size: torch.Size([3072])  - Requires grad: False
Name: bert.encoder.layer.2.output.dense.weight  - Size: torch.Size([768, 3072])  - Requires grad: False
Name: bert.encoder.layer.2.output.dense.bias  - Size: torch.Size([768])  - Requires grad: False
Name: bert.encoder.layer.2.output.LayerNorm.weight  - Size: torch.Size([768])  - Requires grad: False
Name: bert.encoder.layer.2.output.LayerNorm.bias  - Size: torch.Size([768])  - Requires grad: False
Name: bert.encoder.layer.3.attention.self.query.weight  - Size: torch.Size([768, 768])  - Requires grad: False
Name: bert.encoder.layer.3.attention.self.query.bias  - Size: torch.Size([768])  - Requires grad: False
Name: bert.encoder.layer.3.attention.self.key.weight  - Size: torch.Size([768, 768])  - Requires grad: False
Name: bert.encoder.layer.3.attention.self.key.bias  - Size: torch.Size([768])  - Requires grad: False
Name: bert.encoder.layer.3.attention.self.value.weight  - Size: torch.Size([768, 768])  - Requires grad: False
Name: bert.encoder.layer.3.attention.self.value.bias  - Size: torch.Size([768])  - Requires grad: False
Name: bert.encoder.layer.3.attention.output.dense.weight  - Size: torch.Size([768, 768])  - Requires grad: False
Name: bert.encoder.layer.3.attention.output.dense.bias  - Size: torch.Size([768])  - Requires grad: False
Name: bert.encoder.layer.3.attention.output.LayerNorm.weight  - Size: torch.Size([768])  - Requires grad: False
Name: bert.encoder.layer.3.attention.output.LayerNorm.bias  - Size: torch.Size([768])  - Requires grad: False
Name: bert.encoder.layer.3.intermediate.dense.weight  - Size: torch.Size([3072, 768])  - Requires grad: False
Name: bert.encoder.layer.3.intermediate.dense.bias  - Size: torch.Size([3072])  - Requires grad: False
Name: bert.encoder.layer.3.output.dense.weight  - Size: torch.Size([768, 3072])  - Requires grad: False
Name: bert.encoder.layer.3.output.dense.bias  - Size: torch.Size([768])  - Requires grad: False
Name: bert.encoder.layer.3.output.LayerNorm.weight  - Size: torch.Size([768])  - Requires grad: False
Name: bert.encoder.layer.3.output.LayerNorm.bias  - Size: torch.Size([768])  - Requires grad: False
Name: bert.encoder.layer.4.attention.self.query.weight  - Size: torch.Size([768, 768])  - Requires grad: False
Name: bert.encoder.layer.4.attention.self.query.bias  - Size: torch.Size([768])  - Requires grad: False
Name: bert.encoder.layer.4.attention.self.key.weight  - Size: torch.Size([768, 768])  - Requires grad: False
Name: bert.encoder.layer.4.attention.self.key.bias  - Size: torch.Size([768])  - Requires grad: False
Name: bert.encoder.layer.4.attention.self.value.weight  - Size: torch.Size([768, 768])  - Requires grad: False
Name: bert.encoder.layer.4.attention.self.value.bias  - Size: torch.Size([768])  - Requires grad: False
Name: bert.encoder.layer.4.attention.output.dense.weight  - Size: torch.Size([768, 768])  - Requires grad: False
Name: bert.encoder.layer.4.attention.output.dense.bias  - Size: torch.Size([768])  - Requires grad: False
Name: bert.encoder.layer.4.attention.output.LayerNorm.weight  - Size: torch.Size([768])  - Requires grad: False
Name: bert.encoder.layer.4.attention.output.LayerNorm.bias  - Size: torch.Size([768])  - Requires grad: False
Name: bert.encoder.layer.4.intermediate.dense.weight  - Size: torch.Size([3072, 768])  - Requires grad: False
Name: bert.encoder.layer.4.intermediate.dense.bias  - Size: torch.Size([3072])  - Requires grad: False
Name: bert.encoder.layer.4.output.dense.weight  - Size: torch.Size([768, 3072])  - Requires grad: False
Name: bert.encoder.layer.4.output.dense.bias  - Size: torch.Size([768])  - Requires grad: False
Name: bert.encoder.layer.4.output.LayerNorm.weight  - Size: torch.Size([768])  - Requires grad: False
Name: bert.encoder.layer.4.output.LayerNorm.bias  - Size: torch.Size([768])  - Requires grad: False
Name: bert.encoder.layer.5.attention.self.query.weight  - Size: torch.Size([768, 768])  - Requires grad: False
Name: bert.encoder.layer.5.attention.self.query.bias  - Size: torch.Size([768])  - Requires grad: False
Name: bert.encoder.layer.5.attention.self.key.weight  - Size: torch.Size([768, 768])  - Requires grad: False
Name: bert.encoder.layer.5.attention.self.key.bias  - Size: torch.Size([768])  - Requires grad: False
Name: bert.encoder.layer.5.attention.self.value.weight  - Size: torch.Size([768, 768])  - Requires grad: False
Name: bert.encoder.layer.5.attention.self.value.bias  - Size: torch.Size([768])  - Requires grad: False
Name: bert.encoder.layer.5.attention.output.dense.weight  - Size: torch.Size([768, 768])  - Requires grad: False
Name: bert.encoder.layer.5.attention.output.dense.bias  - Size: torch.Size([768])  - Requires grad: False
Name: bert.encoder.layer.5.attention.output.LayerNorm.weight  - Size: torch.Size([768])  - Requires grad: False
Name: bert.encoder.layer.5.attention.output.LayerNorm.bias  - Size: torch.Size([768])  - Requires grad: False
Name: bert.encoder.layer.5.intermediate.dense.weight  - Size: torch.Size([3072, 768])  - Requires grad: False
Name: bert.encoder.layer.5.intermediate.dense.bias  - Size: torch.Size([3072])  - Requires grad: False
Name: bert.encoder.layer.5.output.dense.weight  - Size: torch.Size([768, 3072])  - Requires grad: False
Name: bert.encoder.layer.5.output.dense.bias  - Size: torch.Size([768])  - Requires grad: False
Name: bert.encoder.layer.5.output.LayerNorm.weight  - Size: torch.Size([768])  - Requires grad: False
Name: bert.encoder.layer.5.output.LayerNorm.bias  - Size: torch.Size([768])  - Requires grad: False
Name: bert.encoder.layer.6.attention.self.query.weight  - Size: torch.Size([768, 768])  - Requires grad: False
Name: bert.encoder.layer.6.attention.self.query.bias  - Size: torch.Size([768])  - Requires grad: False
Name: bert.encoder.layer.6.attention.self.key.weight  - Size: torch.Size([768, 768])  - Requires grad: False
Name: bert.encoder.layer.6.attention.self.key.bias  - Size: torch.Size([768])  - Requires grad: False
Name: bert.encoder.layer.6.attention.self.value.weight  - Size: torch.Size([768, 768])  - Requires grad: False
Name: bert.encoder.layer.6.attention.self.value.bias  - Size: torch.Size([768])  - Requires grad: False
Name: bert.encoder.layer.6.attention.output.dense.weight  - Size: torch.Size([768, 768])  - Requires grad: False
Name: bert.encoder.layer.6.attention.output.dense.bias  - Size: torch.Size([768])  - Requires grad: False
Name: bert.encoder.layer.6.attention.output.LayerNorm.weight  - Size: torch.Size([768])  - Requires grad: False
Name: bert.encoder.layer.6.attention.output.LayerNorm.bias  - Size: torch.Size([768])  - Requires grad: False
Name: bert.encoder.layer.6.intermediate.dense.weight  - Size: torch.Size([3072, 768])  - Requires grad: False
Name: bert.encoder.layer.6.intermediate.dense.bias  - Size: torch.Size([3072])  - Requires grad: False
Name: bert.encoder.layer.6.output.dense.weight  - Size: torch.Size([768, 3072])  - Requires grad: False
Name: bert.encoder.layer.6.output.dense.bias  - Size: torch.Size([768])  - Requires grad: False
Name: bert.encoder.layer.6.output.LayerNorm.weight  - Size: torch.Size([768])  - Requires grad: False
Name: bert.encoder.layer.6.output.LayerNorm.bias  - Size: torch.Size([768])  - Requires grad: False
Name: bert.encoder.layer.7.attention.self.query.weight  - Size: torch.Size([768, 768])  - Requires grad: False
Name: bert.encoder.layer.7.attention.self.query.bias  - Size: torch.Size([768])  - Requires grad: False
Name: bert.encoder.layer.7.attention.self.key.weight  - Size: torch.Size([768, 768])  - Requires grad: False
Name: bert.encoder.layer.7.attention.self.key.bias  - Size: torch.Size([768])  - Requires grad: False
Name: bert.encoder.layer.7.attention.self.value.weight  - Size: torch.Size([768, 768])  - Requires grad: False
Name: bert.encoder.layer.7.attention.self.value.bias  - Size: torch.Size([768])  - Requires grad: False
Name: bert.encoder.layer.7.attention.output.dense.weight  - Size: torch.Size([768, 768])  - Requires grad: False
Name: bert.encoder.layer.7.attention.output.dense.bias  - Size: torch.Size([768])  - Requires grad: False
Name: bert.encoder.layer.7.attention.output.LayerNorm.weight  - Size: torch.Size([768])  - Requires grad: False
Name: bert.encoder.layer.7.attention.output.LayerNorm.bias  - Size: torch.Size([768])  - Requires grad: False
Name: bert.encoder.layer.7.intermediate.dense.weight  - Size: torch.Size([3072, 768])  - Requires grad: False
Name: bert.encoder.layer.7.intermediate.dense.bias  - Size: torch.Size([3072])  - Requires grad: False
Name: bert.encoder.layer.7.output.dense.weight  - Size: torch.Size([768, 3072])  - Requires grad: False
Name: bert.encoder.layer.7.output.dense.bias  - Size: torch.Size([768])  - Requires grad: False
Name: bert.encoder.layer.7.output.LayerNorm.weight  - Size: torch.Size([768])  - Requires grad: False
Name: bert.encoder.layer.7.output.LayerNorm.bias  - Size: torch.Size([768])  - Requires grad: False
Name: bert.encoder.layer.8.attention.self.query.weight  - Size: torch.Size([768, 768])  - Requires grad: False
Name: bert.encoder.layer.8.attention.self.query.bias  - Size: torch.Size([768])  - Requires grad: False
Name: bert.encoder.layer.8.attention.self.key.weight  - Size: torch.Size([768, 768])  - Requires grad: False
Name: bert.encoder.layer.8.attention.self.key.bias  - Size: torch.Size([768])  - Requires grad: False
Name: bert.encoder.layer.8.attention.self.value.weight  - Size: torch.Size([768, 768])  - Requires grad: False
Name: bert.encoder.layer.8.attention.self.value.bias  - Size: torch.Size([768])  - Requires grad: False
Name: bert.encoder.layer.8.attention.output.dense.weight  - Size: torch.Size([768, 768])  - Requires grad: False
Name: bert.encoder.layer.8.attention.output.dense.bias  - Size: torch.Size([768])  - Requires grad: False
Name: bert.encoder.layer.8.attention.output.LayerNorm.weight  - Size: torch.Size([768])  - Requires grad: False
Name: bert.encoder.layer.8.attention.output.LayerNorm.bias  - Size: torch.Size([768])  - Requires grad: False
Name: bert.encoder.layer.8.intermediate.dense.weight  - Size: torch.Size([3072, 768])  - Requires grad: False
Name: bert.encoder.layer.8.intermediate.dense.bias  - Size: torch.Size([3072])  - Requires grad: False
Name: bert.encoder.layer.8.output.dense.weight  - Size: torch.Size([768, 3072])  - Requires grad: False
Name: bert.encoder.layer.8.output.dense.bias  - Size: torch.Size([768])  - Requires grad: False
Name: bert.encoder.layer.8.output.LayerNorm.weight  - Size: torch.Size([768])  - Requires grad: False
Name: bert.encoder.layer.8.output.LayerNorm.bias  - Size: torch.Size([768])  - Requires grad: False
Name: bert.encoder.layer.9.attention.self.query.weight  - Size: torch.Size([768, 768])  - Requires grad: False
Name: bert.encoder.layer.9.attention.self.query.bias  - Size: torch.Size([768])  - Requires grad: False
Name: bert.encoder.layer.9.attention.self.key.weight  - Size: torch.Size([768, 768])  - Requires grad: False
Name: bert.encoder.layer.9.attention.self.key.bias  - Size: torch.Size([768])  - Requires grad: False
Name: bert.encoder.layer.9.attention.self.value.weight  - Size: torch.Size([768, 768])  - Requires grad: False
Name: bert.encoder.layer.9.attention.self.value.bias  - Size: torch.Size([768])  - Requires grad: False
Name: bert.encoder.layer.9.attention.output.dense.weight  - Size: torch.Size([768, 768])  - Requires grad: False
Name: bert.encoder.layer.9.attention.output.dense.bias  - Size: torch.Size([768])  - Requires grad: False
Name: bert.encoder.layer.9.attention.output.LayerNorm.weight  - Size: torch.Size([768])  - Requires grad: False
Name: bert.encoder.layer.9.attention.output.LayerNorm.bias  - Size: torch.Size([768])  - Requires grad: False
Name: bert.encoder.layer.9.intermediate.dense.weight  - Size: torch.Size([3072, 768])  - Requires grad: False
Name: bert.encoder.layer.9.intermediate.dense.bias  - Size: torch.Size([3072])  - Requires grad: False
Name: bert.encoder.layer.9.output.dense.weight  - Size: torch.Size([768, 3072])  - Requires grad: False
Name: bert.encoder.layer.9.output.dense.bias  - Size: torch.Size([768])  - Requires grad: False
Name: bert.encoder.layer.9.output.LayerNorm.weight  - Size: torch.Size([768])  - Requires grad: False
Name: bert.encoder.layer.9.output.LayerNorm.bias  - Size: torch.Size([768])  - Requires grad: False
Name: bert.encoder.layer.10.attention.self.query.weight  - Size: torch.Size([768, 768])  - Requires grad: False
Name: bert.encoder.layer.10.attention.self.query.bias  - Size: torch.Size([768])  - Requires grad: False
Name: bert.encoder.layer.10.attention.self.key.weight  - Size: torch.Size([768, 768])  - Requires grad: False
Name: bert.encoder.layer.10.attention.self.key.bias  - Size: torch.Size([768])  - Requires grad: False
Name: bert.encoder.layer.10.attention.self.value.weight  - Size: torch.Size([768, 768])  - Requires grad: False
Name: bert.encoder.layer.10.attention.self.value.bias  - Size: torch.Size([768])  - Requires grad: False
Name: bert.encoder.layer.10.attention.output.dense.weight  - Size: torch.Size([768, 768])  - Requires grad: False
Name: bert.encoder.layer.10.attention.output.dense.bias  - Size: torch.Size([768])  - Requires grad: False
Name: bert.encoder.layer.10.attention.output.LayerNorm.weight  - Size: torch.Size([768])  - Requires grad: False
Name: bert.encoder.layer.10.attention.output.LayerNorm.bias  - Size: torch.Size([768])  - Requires grad: False
Name: bert.encoder.layer.10.intermediate.dense.weight  - Size: torch.Size([3072, 768])  - Requires grad: False
Name: bert.encoder.layer.10.intermediate.dense.bias  - Size: torch.Size([3072])  - Requires grad: False
Name: bert.encoder.layer.10.output.dense.weight  - Size: torch.Size([768, 3072])  - Requires grad: False
Name: bert.encoder.layer.10.output.dense.bias  - Size: torch.Size([768])  - Requires grad: False
Name: bert.encoder.layer.10.output.LayerNorm.weight  - Size: torch.Size([768])  - Requires grad: False
Name: bert.encoder.layer.10.output.LayerNorm.bias  - Size: torch.Size([768])  - Requires grad: False
Name: bert.encoder.layer.11.attention.self.query.weight  - Size: torch.Size([768, 768])  - Requires grad: True
Name: bert.encoder.layer.11.attention.self.query.bias  - Size: torch.Size([768])  - Requires grad: True
Name: bert.encoder.layer.11.attention.self.key.weight  - Size: torch.Size([768, 768])  - Requires grad: True
Name: bert.encoder.layer.11.attention.self.key.bias  - Size: torch.Size([768])  - Requires grad: True
Name: bert.encoder.layer.11.attention.self.value.weight  - Size: torch.Size([768, 768])  - Requires grad: True
Name: bert.encoder.layer.11.attention.self.value.bias  - Size: torch.Size([768])  - Requires grad: True
Name: bert.encoder.layer.11.attention.output.dense.weight  - Size: torch.Size([768, 768])  - Requires grad: True
Name: bert.encoder.layer.11.attention.output.dense.bias  - Size: torch.Size([768])  - Requires grad: True
Name: bert.encoder.layer.11.attention.output.LayerNorm.weight  - Size: torch.Size([768])  - Requires grad: True
Name: bert.encoder.layer.11.attention.output.LayerNorm.bias  - Size: torch.Size([768])  - Requires grad: True
Name: bert.encoder.layer.11.intermediate.dense.weight  - Size: torch.Size([3072, 768])  - Requires grad: True
Name: bert.encoder.layer.11.intermediate.dense.bias  - Size: torch.Size([3072])  - Requires grad: True
Name: bert.encoder.layer.11.output.dense.weight  - Size: torch.Size([768, 3072])  - Requires grad: True
Name: bert.encoder.layer.11.output.dense.bias  - Size: torch.Size([768])  - Requires grad: True
Name: bert.encoder.layer.11.output.LayerNorm.weight  - Size: torch.Size([768])  - Requires grad: True
Name: bert.encoder.layer.11.output.LayerNorm.bias  - Size: torch.Size([768])  - Requires grad: True
Name: bert.pooler.dense.weight  - Size: torch.Size([768, 768])  - Requires grad: True
Name: bert.pooler.dense.bias  - Size: torch.Size([768])  - Requires grad: True
Name: classifier.weight  - Size: torch.Size([28, 768])  - Requires grad: True
Name: classifier.bias  - Size: torch.Size([28])  - Requires grad: True
Fine-tuning the model on the GoEmotions dataset...
Filtered dataset:
 DatasetDict({
    train: Dataset({
        features: ['text', 'labels', 'id'],
        num_rows: 36308
    })
    validation: Dataset({
        features: ['text', 'labels', 'id'],
        num_rows: 4548
    })
    test: Dataset({
        features: ['text', 'labels', 'id'],
        num_rows: 4590
    })
})
{'loss': 1.7896, 'grad_norm': 4.517978191375732, 'learning_rate': 0.000925990675990676, 'epoch': 1.0}
{'eval_loss': 1.5113416910171509, 'eval_accuracy': 0.5560686015831134, 'eval_f1': 0.3467227536276369, 'eval_classification_report': '[String too long to display, length: 1794 > 100. Consider increasing `max_str_len` if needed.]', 'eval_runtime': 10.4386, 'eval_samples_per_second': 435.69, 'eval_steps_per_second': 13.699, 'epoch': 1.0}
{'loss': 1.5393, 'grad_norm': 4.144822597503662, 'learning_rate': 0.0007937062937062938, 'epoch': 2.0}
{'eval_loss': 1.4725769758224487, 'eval_accuracy': 0.5657431838170625, 'eval_f1': 0.3837733060525316, 'eval_classification_report': '[String too long to display, length: 1794 > 100. Consider increasing `max_str_len` if needed.]', 'eval_runtime': 9.2676, 'eval_samples_per_second': 490.74, 'eval_steps_per_second': 15.43, 'epoch': 2.0}
{'loss': 1.4665, 'grad_norm': 3.35322904586792, 'learning_rate': 0.0006614219114219114, 'epoch': 3.0}
{'eval_loss': 1.382096529006958, 'eval_accuracy': 0.5848724714160071, 'eval_f1': 0.37859153487330455, 'eval_classification_report': '[String too long to display, length: 1794 > 100. Consider increasing `max_str_len` if needed.]', 'eval_runtime': 9.5419, 'eval_samples_per_second': 476.635, 'eval_steps_per_second': 14.987, 'epoch': 3.0}
{'loss': 1.4129, 'grad_norm': 3.2183728218078613, 'learning_rate': 0.0005291375291375291, 'epoch': 4.0}
{'eval_loss': 1.355842113494873, 'eval_accuracy': 0.5960861917326298, 'eval_f1': 0.4331961031469219, 'eval_classification_report': '[String too long to display, length: 1794 > 100. Consider increasing `max_str_len` if needed.]', 'eval_runtime': 17.4997, 'eval_samples_per_second': 259.891, 'eval_steps_per_second': 8.172, 'epoch': 4.0}
{'loss': 1.3563, 'grad_norm': 4.044126033782959, 'learning_rate': 0.0003968531468531469, 'epoch': 5.0}
{'eval_loss': 1.3744018077850342, 'eval_accuracy': 0.5842128408091469, 'eval_f1': 0.40709260708465295, 'eval_classification_report': '[String too long to display, length: 1794 > 100. Consider increasing `max_str_len` if needed.]', 'eval_runtime': 10.8128, 'eval_samples_per_second': 420.613, 'eval_steps_per_second': 13.225, 'epoch': 5.0}
{'loss': 1.2927, 'grad_norm': 3.6080007553100586, 'learning_rate': 0.00026456876456876455, 'epoch': 6.0}
{'eval_loss': 1.3141041994094849, 'eval_accuracy': 0.6068601583113457, 'eval_f1': 0.4521312441289594, 'eval_classification_report': '[String too long to display, length: 1794 > 100. Consider increasing `max_str_len` if needed.]', 'eval_runtime': 9.372, 'eval_samples_per_second': 485.277, 'eval_steps_per_second': 15.258, 'epoch': 6.0}
{'loss': 1.22, 'grad_norm': 3.122204542160034, 'learning_rate': 0.00013228438228438227, 'epoch': 7.0}
{'eval_loss': 1.2712992429733276, 'eval_accuracy': 0.6134564643799473, 'eval_f1': 0.46169189488936624, 'eval_classification_report': '[String too long to display, length: 1794 > 100. Consider increasing `max_str_len` if needed.]', 'eval_runtime': 9.6036, 'eval_samples_per_second': 473.573, 'eval_steps_per_second': 14.89, 'epoch': 7.0}
{'loss': 1.1236, 'grad_norm': 3.758175849914551, 'learning_rate': 0.0, 'epoch': 8.0}
{'eval_loss': 1.2608574628829956, 'eval_accuracy': 0.6125769569041337, 'eval_f1': 0.46426162585758846, 'eval_classification_report': '[String too long to display, length: 1794 > 100. Consider increasing `max_str_len` if needed.]', 'eval_runtime': 10.0485, 'eval_samples_per_second': 452.606, 'eval_steps_per_second': 14.231, 'epoch': 8.0}
{'train_runtime': 932.6827, 'train_samples_per_second': 311.429, 'train_steps_per_second': 9.735, 'train_loss': 1.40011603401621, 'epoch': 8.0}
Saving classifier, encoder, and pooler layers
Gathered layers to save.
Layer weights saved.
Predictions shape:  (4590, 28) 
Labels shape:  (4590, 1) 
Class Predictions:  [25 14 20 ... 27  0 27]
[np.int64(1615), 'Thank you for your service', 'gratitude', 'gratitude']
[np.int64(3174), '[NAME] x [NAME] sounds cute for some reason.', 'admiration', 'admiration']
[np.int64(23), "What's your source for that? Just curious (and yes I know it sounds like a tired contrarian statement).", 'curiosity', 'curiosity']
[np.int64(4586), 'Well that makes sense.', 'approval', 'approval']
[np.int64(1745), 'You still didn’t answer the question. Sorry it’s bugging me :)', 'remorse', 'remorse']
[np.int64(691), 'Do not use logic dude. Not cool.', 'disapproval', 'disapproval']
[np.int64(1047), "You conveniently aren't speaking to my assertion.", 'neutral', 'neutral']
[np.int64(2031), "This hits hard, sadly both of my relationships have been toxic so I've decided not to start any new ones until I start going to uni ", 'sadness', 'sadness']
[np.int64(354), 'Sorry your right', 'remorse', 'remorse']
[np.int64(2034), 'SHE SHOULD BE ON A HILL SOMEWHERE WITH THE SUN AND THE CLOUDS ABOVE HER!', 'neutral', 'neutral']
[np.int64(745), 'Yeah I wish they could just finish 9th every year in the conference', 'optimism', 'desire']
[np.int64(1863), 'I do not trust our defense at all. I do not believe you can increase the range of Andujar.', 'surprise', 'disapproval']
[np.int64(1150), 'Difficult to see how this video wouldn’t have been much more interesting if properly shot in landscape.', 'disapproval', 'confusion']
[np.int64(2475), 'For the people and finding out how they end up? Sure. Other than that, it’s not great. Frustrating at best. ', 'anger', 'curiosity']
[np.int64(2788), 'RIGHT?! My mom would literally never forgive the bitch 😂', 'amusement', 'anger']
[np.int64(1376), 'At my job I can/Do get laid a lot so the second one!!', 'approval', 'neutral']
[np.int64(3261), "I wouldn't say age would stop it just lessen the affect of it but probably start to notice it around mid to late 30's.", 'realization', 'neutral']
[np.int64(3143), "Good luck for everyone pronouncing [NAME] name. If he keeps scoring like this, i'll have fun listening some commentators around NHL.", 'gratitude', 'joy']
[np.int64(1629), "Couldn't have happened to a nicer person.", 'sadness', 'disapproval']
[np.int64(3139), 'I plan on being respectful lol. I hope he’s there.', 'optimism', 'amusement']
Correct Instances:  [[np.int64(1615), 'Thank you for your service', 'gratitude', 'gratitude'], [np.int64(3174), '[NAME] x [NAME] sounds cute for some reason.', 'admiration', 'admiration'], [np.int64(23), "What's your source for that? Just curious (and yes I know it sounds like a tired contrarian statement).", 'curiosity', 'curiosity'], [np.int64(4586), 'Well that makes sense.', 'approval', 'approval'], [np.int64(1745), 'You still didn’t answer the question. Sorry it’s bugging me :)', 'remorse', 'remorse'], [np.int64(691), 'Do not use logic dude. Not cool.', 'disapproval', 'disapproval'], [np.int64(1047), "You conveniently aren't speaking to my assertion.", 'neutral', 'neutral'], [np.int64(2031), "This hits hard, sadly both of my relationships have been toxic so I've decided not to start any new ones until I start going to uni ", 'sadness', 'sadness'], [np.int64(354), 'Sorry your right', 'remorse', 'remorse'], [np.int64(2034), 'SHE SHOULD BE ON A HILL SOMEWHERE WITH THE SUN AND THE CLOUDS ABOVE HER!', 'neutral', 'neutral']]
Incorrect Instances:  [[np.int64(745), 'Yeah I wish they could just finish 9th every year in the conference', 'optimism', 'desire'], [np.int64(1863), 'I do not trust our defense at all. I do not believe you can increase the range of Andujar.', 'surprise', 'disapproval'], [np.int64(1150), 'Difficult to see how this video wouldn’t have been much more interesting if properly shot in landscape.', 'disapproval', 'confusion'], [np.int64(2475), 'For the people and finding out how they end up? Sure. Other than that, it’s not great. Frustrating at best. ', 'anger', 'curiosity'], [np.int64(2788), 'RIGHT?! My mom would literally never forgive the bitch 😂', 'amusement', 'anger'], [np.int64(1376), 'At my job I can/Do get laid a lot so the second one!!', 'approval', 'neutral'], [np.int64(3261), "I wouldn't say age would stop it just lessen the affect of it but probably start to notice it around mid to late 30's.", 'realization', 'neutral'], [np.int64(3143), "Good luck for everyone pronouncing [NAME] name. If he keeps scoring like this, i'll have fun listening some commentators around NHL.", 'gratitude', 'joy'], [np.int64(1629), "Couldn't have happened to a nicer person.", 'sadness', 'disapproval'], [np.int64(3139), 'I plan on being respectful lol. I hope he’s there.', 'optimism', 'amusement']]
Metrics:
F1: 0.4988184910840473 
Accuracy: 0.6152505446623093
Results: {'f1': [np.float64(0.3467227536276369), np.float64(0.3837733060525316), np.float64(0.37859153487330455), np.float64(0.4331961031469219), np.float64(0.40709260708465295), np.float64(0.4521312441289594), np.float64(0.46169189488936624), np.float64(0.46426162585758846), np.float64(0.4988184910840473)], 'accuracy': [0.5560686015831134, 0.5657431838170625, 0.5848724714160071, 0.5960861917326298, 0.5842128408091469, 0.6068601583113457, 0.6134564643799473, 0.6125769569041337, 0.6152505446623093], 'duration': [126.3958649635315, 231.73951506614685, 336.37128496170044, 455.9537241458893, 630.7906091213226, 733.8259079456329, 834.9648039340973, 938.3352959156036, 949.2227461338043], 'reports': ['                precision    recall  f1-score   support\n\n    admiration       0.54      0.83      0.65       326\n     amusement       0.68      0.87      0.76       208\n         anger       0.47      0.08      0.14       109\n     annoyance       0.22      0.43      0.29       164\n      approval       0.59      0.18      0.28       258\n        caring       0.29      0.04      0.07        96\n     confusion       0.27      0.30      0.29       102\n     curiosity       0.47      0.46      0.47       164\n        desire       0.65      0.46      0.54        52\ndisappointment       0.33      0.01      0.02        91\n   disapproval       0.36      0.36      0.36       212\n       disgust       0.43      0.10      0.16        61\n embarrassment       0.86      0.30      0.44        20\n    excitement       1.00      0.06      0.11        52\n          fear       0.93      0.24      0.38        58\n     gratitude       0.95      0.88      0.91       261\n         grief       0.00      0.00      0.00         6\n           joy       0.58      0.48      0.53       106\n          love       0.67      0.88      0.76       173\n   nervousness       0.00      0.00      0.00         8\n      optimism       0.82      0.28      0.42       119\n         pride       0.00      0.00      0.00         9\n   realization       0.00      0.00      0.00        74\n        relief       0.00      0.00      0.00         8\n       remorse       0.55      0.85      0.67        40\n       sadness       0.37      0.61      0.46        84\n      surprise       0.57      0.25      0.35        95\n       neutral       0.59      0.71      0.64      1592\n\n      accuracy                           0.56      4548\n     macro avg       0.47      0.35      0.35      4548\n  weighted avg       0.56      0.56      0.52      4548\n', '                precision    recall  f1-score   support\n\n    admiration       0.79      0.58      0.67       326\n     amusement       0.68      0.88      0.77       208\n         anger       0.43      0.55      0.48       109\n     annoyance       0.44      0.14      0.21       164\n      approval       0.70      0.08      0.15       258\n        caring       0.00      0.00      0.00        96\n     confusion       0.38      0.16      0.22       102\n     curiosity       0.42      0.61      0.50       164\n        desire       0.83      0.29      0.43        52\ndisappointment       0.22      0.07      0.10        91\n   disapproval       0.30      0.53      0.38       212\n       disgust       0.45      0.39      0.42        61\n embarrassment       0.75      0.30      0.43        20\n    excitement       0.80      0.08      0.14        52\n          fear       0.67      0.52      0.58        58\n     gratitude       0.97      0.85      0.91       261\n         grief       0.00      0.00      0.00         6\n           joy       0.50      0.58      0.54       106\n          love       0.70      0.87      0.78       173\n   nervousness       0.00      0.00      0.00         8\n      optimism       0.75      0.50      0.60       119\n         pride       0.00      0.00      0.00         9\n   realization       0.26      0.27      0.26        74\n        relief       0.00      0.00      0.00         8\n       remorse       0.61      0.75      0.67        40\n       sadness       0.61      0.40      0.49        84\n      surprise       0.61      0.28      0.39        95\n       neutral       0.56      0.74      0.63      1592\n\n      accuracy                           0.57      4548\n     macro avg       0.48      0.37      0.38      4548\n  weighted avg       0.57      0.57      0.53      4548\n', '                precision    recall  f1-score   support\n\n    admiration       0.62      0.79      0.70       326\n     amusement       0.71      0.79      0.75       208\n         anger       0.31      0.57      0.40       109\n     annoyance       0.31      0.09      0.14       164\n      approval       0.58      0.16      0.24       258\n        caring       0.52      0.12      0.20        96\n     confusion       0.61      0.14      0.22       102\n     curiosity       0.51      0.26      0.35       164\n        desire       0.67      0.58      0.62        52\ndisappointment       0.00      0.00      0.00        91\n   disapproval       0.41      0.11      0.18       212\n       disgust       0.38      0.41      0.39        61\n embarrassment       0.71      0.25      0.37        20\n    excitement       0.30      0.25      0.27        52\n          fear       0.60      0.59      0.59        58\n     gratitude       0.87      0.92      0.90       261\n         grief       0.00      0.00      0.00         6\n           joy       0.66      0.37      0.47       106\n          love       0.65      0.91      0.76       173\n   nervousness       0.00      0.00      0.00         8\n      optimism       0.71      0.47      0.57       119\n         pride       0.00      0.00      0.00         9\n   realization       0.69      0.15      0.24        74\n        relief       0.00      0.00      0.00         8\n       remorse       0.65      0.60      0.62        40\n       sadness       0.60      0.37      0.46        84\n      surprise       0.63      0.39      0.48        95\n       neutral       0.56      0.83      0.67      1592\n\n      accuracy                           0.58      4548\n     macro avg       0.47      0.36      0.38      4548\n  weighted avg       0.56      0.58      0.54      4548\n', '                precision    recall  f1-score   support\n\n    admiration       0.68      0.76      0.72       326\n     amusement       0.78      0.76      0.77       208\n         anger       0.52      0.40      0.45       109\n     annoyance       0.35      0.18      0.24       164\n      approval       0.51      0.22      0.31       258\n        caring       0.64      0.19      0.29        96\n     confusion       0.61      0.25      0.35       102\n     curiosity       0.40      0.63      0.49       164\n        desire       0.52      0.60      0.55        52\ndisappointment       0.32      0.09      0.14        91\n   disapproval       0.43      0.29      0.35       212\n       disgust       0.31      0.64      0.42        61\n embarrassment       0.62      0.40      0.48        20\n    excitement       0.32      0.31      0.31        52\n          fear       0.64      0.59      0.61        58\n     gratitude       0.95      0.89      0.92       261\n         grief       0.00      0.00      0.00         6\n           joy       0.60      0.52      0.56       106\n          love       0.67      0.90      0.76       173\n   nervousness       0.50      0.12      0.20         8\n      optimism       0.60      0.55      0.58       119\n         pride       1.00      0.22      0.36         9\n   realization       0.80      0.05      0.10        74\n        relief       0.00      0.00      0.00         8\n       remorse       0.57      0.50      0.53        40\n       sadness       0.39      0.63      0.48        84\n      surprise       0.52      0.42      0.47        95\n       neutral       0.61      0.75      0.67      1592\n\n      accuracy                           0.60      4548\n     macro avg       0.53      0.42      0.43      4548\n  weighted avg       0.59      0.60      0.57      4548\n', '                precision    recall  f1-score   support\n\n    admiration       0.45      0.85      0.59       326\n     amusement       0.70      0.80      0.75       208\n         anger       0.40      0.62      0.49       109\n     annoyance       0.38      0.12      0.18       164\n      approval       0.68      0.15      0.25       258\n        caring       0.56      0.33      0.42        96\n     confusion       0.54      0.27      0.36       102\n     curiosity       0.50      0.35      0.41       164\n        desire       0.74      0.48      0.58        52\ndisappointment       0.25      0.03      0.06        91\n   disapproval       0.53      0.24      0.33       212\n       disgust       0.47      0.38      0.42        61\n embarrassment       0.83      0.25      0.38        20\n    excitement       0.53      0.17      0.26        52\n          fear       0.49      0.64      0.56        58\n     gratitude       0.92      0.91      0.91       261\n         grief       0.00      0.00      0.00         6\n           joy       0.77      0.23      0.35       106\n          love       0.73      0.80      0.76       173\n   nervousness       0.50      0.12      0.20         8\n      optimism       0.65      0.54      0.59       119\n         pride       0.00      0.00      0.00         9\n   realization       0.54      0.18      0.27        74\n        relief       0.00      0.00      0.00         8\n       remorse       0.70      0.47      0.57        40\n       sadness       0.44      0.58      0.50        84\n      surprise       0.52      0.60      0.56        95\n       neutral       0.59      0.76      0.67      1592\n\n      accuracy                           0.58      4548\n     macro avg       0.51      0.39      0.41      4548\n  weighted avg       0.59      0.58      0.55      4548\n', '                precision    recall  f1-score   support\n\n    admiration       0.63      0.82      0.71       326\n     amusement       0.66      0.91      0.76       208\n         anger       0.47      0.52      0.49       109\n     annoyance       0.44      0.14      0.21       164\n      approval       0.53      0.24      0.33       258\n        caring       0.49      0.44      0.46        96\n     confusion       0.46      0.31      0.37       102\n     curiosity       0.42      0.74      0.54       164\n        desire       0.56      0.48      0.52        52\ndisappointment       0.25      0.18      0.21        91\n   disapproval       0.63      0.20      0.31       212\n       disgust       0.49      0.48      0.48        61\n embarrassment       0.60      0.45      0.51        20\n    excitement       0.30      0.37      0.33        52\n          fear       0.82      0.48      0.61        58\n     gratitude       0.96      0.89      0.92       261\n         grief       0.00      0.00      0.00         6\n           joy       0.58      0.58      0.58       106\n          love       0.74      0.88      0.81       173\n   nervousness       0.00      0.00      0.00         8\n      optimism       0.66      0.55      0.60       119\n         pride       1.00      0.11      0.20         9\n   realization       0.45      0.26      0.33        74\n        relief       0.00      0.00      0.00         8\n       remorse       0.57      0.65      0.60        40\n       sadness       0.50      0.60      0.54        84\n      surprise       0.53      0.60      0.56        95\n       neutral       0.63      0.71      0.67      1592\n\n      accuracy                           0.61      4548\n     macro avg       0.51      0.45      0.45      4548\n  weighted avg       0.60      0.61      0.59      4548\n', '                precision    recall  f1-score   support\n\n    admiration       0.65      0.83      0.73       326\n     amusement       0.72      0.92      0.81       208\n         anger       0.47      0.47      0.47       109\n     annoyance       0.29      0.21      0.24       164\n      approval       0.53      0.27      0.35       258\n        caring       0.60      0.35      0.44        96\n     confusion       0.49      0.32      0.39       102\n     curiosity       0.42      0.38      0.40       164\n        desire       0.63      0.56      0.59        52\ndisappointment       0.33      0.14      0.20        91\n   disapproval       0.51      0.29      0.37       212\n       disgust       0.44      0.51      0.47        61\n embarrassment       0.57      0.40      0.47        20\n    excitement       0.54      0.25      0.34        52\n          fear       0.58      0.71      0.64        58\n     gratitude       0.94      0.90      0.92       261\n         grief       0.00      0.00      0.00         6\n           joy       0.59      0.59      0.59       106\n          love       0.71      0.88      0.78       173\n   nervousness       1.00      0.12      0.22         8\n      optimism       0.61      0.59      0.60       119\n         pride       1.00      0.11      0.20         9\n   realization       0.46      0.16      0.24        74\n        relief       0.00      0.00      0.00         8\n       remorse       0.62      0.70      0.66        40\n       sadness       0.53      0.58      0.55        84\n      surprise       0.47      0.65      0.55        95\n       neutral       0.63      0.74      0.68      1592\n\n      accuracy                           0.61      4548\n     macro avg       0.55      0.45      0.46      4548\n  weighted avg       0.60      0.61      0.59      4548\n', '                precision    recall  f1-score   support\n\n    admiration       0.69      0.80      0.74       326\n     amusement       0.71      0.89      0.79       208\n         anger       0.44      0.52      0.48       109\n     annoyance       0.34      0.23      0.27       164\n      approval       0.51      0.28      0.36       258\n        caring       0.52      0.45      0.48        96\n     confusion       0.49      0.33      0.40       102\n     curiosity       0.43      0.50      0.46       164\n        desire       0.61      0.52      0.56        52\ndisappointment       0.30      0.18      0.22        91\n   disapproval       0.48      0.33      0.40       212\n       disgust       0.43      0.49      0.46        61\n embarrassment       0.60      0.45      0.51        20\n    excitement       0.47      0.31      0.37        52\n          fear       0.62      0.57      0.59        58\n     gratitude       0.93      0.90      0.91       261\n         grief       0.00      0.00      0.00         6\n           joy       0.61      0.56      0.58       106\n          love       0.71      0.90      0.79       173\n   nervousness       1.00      0.12      0.22         8\n      optimism       0.65      0.61      0.63       119\n         pride       1.00      0.11      0.20         9\n   realization       0.54      0.18      0.27        74\n        relief       0.00      0.00      0.00         8\n       remorse       0.57      0.57      0.57        40\n       sadness       0.44      0.63      0.52        84\n      surprise       0.52      0.54      0.53        95\n       neutral       0.64      0.72      0.68      1592\n\n      accuracy                           0.61      4548\n     macro avg       0.54      0.45      0.46      4548\n  weighted avg       0.60      0.61      0.60      4548\n', '                precision    recall  f1-score   support\n\n    admiration       0.63      0.72      0.67       348\n     amusement       0.73      0.89      0.80       186\n         anger       0.49      0.44      0.46       131\n     annoyance       0.36      0.23      0.28       194\n      approval       0.54      0.36      0.44       236\n        caring       0.50      0.43      0.46        86\n     confusion       0.41      0.37      0.39        97\n     curiosity       0.48      0.53      0.50       176\n        desire       0.58      0.38      0.46        56\ndisappointment       0.42      0.25      0.31        88\n   disapproval       0.41      0.34      0.37       195\n       disgust       0.54      0.50      0.52        76\n embarrassment       0.62      0.43      0.51        23\n    excitement       0.50      0.42      0.46        57\n          fear       0.73      0.74      0.73        65\n     gratitude       0.91      0.90      0.90       260\n         grief       0.00      0.00      0.00         2\n           joy       0.53      0.58      0.56        93\n          love       0.74      0.88      0.80       160\n   nervousness       0.64      0.58      0.61        12\n      optimism       0.61      0.61      0.61       107\n         pride       0.40      0.29      0.33         7\n   realization       0.41      0.12      0.19        89\n        relief       0.50      0.14      0.22         7\n       remorse       0.57      0.75      0.65        44\n       sadness       0.56      0.59      0.57       102\n      surprise       0.47      0.44      0.46        87\n       neutral       0.65      0.73      0.69      1606\n\n      accuracy                           0.62      4590\n     macro avg       0.53      0.49      0.50      4590\n  weighted avg       0.60      0.62      0.60      4590\n'], 'final': {'f1': np.float64(0.4988184910840473), 'accuracy': 0.6152505446623093, 'duration': 949.2925798892975, 'report': '                precision    recall  f1-score   support\n\n    admiration       0.63      0.72      0.67       348\n     amusement       0.73      0.89      0.80       186\n         anger       0.49      0.44      0.46       131\n     annoyance       0.36      0.23      0.28       194\n      approval       0.54      0.36      0.44       236\n        caring       0.50      0.43      0.46        86\n     confusion       0.41      0.37      0.39        97\n     curiosity       0.48      0.53      0.50       176\n        desire       0.58      0.38      0.46        56\ndisappointment       0.42      0.25      0.31        88\n   disapproval       0.41      0.34      0.37       195\n       disgust       0.54      0.50      0.52        76\n embarrassment       0.62      0.43      0.51        23\n    excitement       0.50      0.42      0.46        57\n          fear       0.73      0.74      0.73        65\n     gratitude       0.91      0.90      0.90       260\n         grief       0.00      0.00      0.00         2\n           joy       0.53      0.58      0.56        93\n          love       0.74      0.88      0.80       160\n   nervousness       0.64      0.58      0.61        12\n      optimism       0.61      0.61      0.61       107\n         pride       0.40      0.29      0.33         7\n   realization       0.41      0.12      0.19        89\n        relief       0.50      0.14      0.22         7\n       remorse       0.57      0.75      0.65        44\n       sadness       0.56      0.59      0.57       102\n      surprise       0.47      0.44      0.46        87\n       neutral       0.65      0.73      0.69      1606\n\n      accuracy                           0.62      4590\n     macro avg       0.53      0.49      0.50      4590\n  weighted avg       0.60      0.62      0.60      4590\n'}}
Final Report:
                 precision    recall  f1-score   support

    admiration       0.63      0.72      0.67       348
     amusement       0.73      0.89      0.80       186
         anger       0.49      0.44      0.46       131
     annoyance       0.36      0.23      0.28       194
      approval       0.54      0.36      0.44       236
        caring       0.50      0.43      0.46        86
     confusion       0.41      0.37      0.39        97
     curiosity       0.48      0.53      0.50       176
        desire       0.58      0.38      0.46        56
disappointment       0.42      0.25      0.31        88
   disapproval       0.41      0.34      0.37       195
       disgust       0.54      0.50      0.52        76
 embarrassment       0.62      0.43      0.51        23
    excitement       0.50      0.42      0.46        57
          fear       0.73      0.74      0.73        65
     gratitude       0.91      0.90      0.90       260
         grief       0.00      0.00      0.00         2
           joy       0.53      0.58      0.56        93
          love       0.74      0.88      0.80       160
   nervousness       0.64      0.58      0.61        12
      optimism       0.61      0.61      0.61       107
         pride       0.40      0.29      0.33         7
   realization       0.41      0.12      0.19        89
        relief       0.50      0.14      0.22         7
       remorse       0.57      0.75      0.65        44
       sadness       0.56      0.59      0.57       102
      surprise       0.47      0.44      0.46        87
       neutral       0.65      0.73      0.69      1606

      accuracy                           0.62      4590
     macro avg       0.53      0.49      0.50      4590
  weighted avg       0.60      0.62      0.60      4590

F1 scores: [np.float64(0.3467227536276369), np.float64(0.3837733060525316), np.float64(0.37859153487330455), np.float64(0.4331961031469219), np.float64(0.40709260708465295), np.float64(0.4521312441289594), np.float64(0.46169189488936624), np.float64(0.46426162585758846), np.float64(0.4988184910840473)]
Accuracies: [0.5560686015831134, 0.5657431838170625, 0.5848724714160071, 0.5960861917326298, 0.5842128408091469, 0.6068601583113457, 0.6134564643799473, 0.6125769569041337, 0.6152505446623093]
Durations: [126.3958649635315, 231.73951506614685, 336.37128496170044, 455.9537241458893, 630.7906091213226, 733.8259079456329, 834.9648039340973, 938.3352959156036, 949.2227461338043]
log_history: [{'loss': 1.7896, 'grad_norm': 4.517978191375732, 'learning_rate': 0.000925990675990676, 'epoch': 1.0, 'step': 1135}, {'eval_loss': 1.5113416910171509, 'eval_accuracy': 0.5560686015831134, 'eval_f1': 0.3467227536276369, 'eval_classification_report': '                precision    recall  f1-score   support\n\n    admiration       0.54      0.83      0.65       326\n     amusement       0.68      0.87      0.76       208\n         anger       0.47      0.08      0.14       109\n     annoyance       0.22      0.43      0.29       164\n      approval       0.59      0.18      0.28       258\n        caring       0.29      0.04      0.07        96\n     confusion       0.27      0.30      0.29       102\n     curiosity       0.47      0.46      0.47       164\n        desire       0.65      0.46      0.54        52\ndisappointment       0.33      0.01      0.02        91\n   disapproval       0.36      0.36      0.36       212\n       disgust       0.43      0.10      0.16        61\n embarrassment       0.86      0.30      0.44        20\n    excitement       1.00      0.06      0.11        52\n          fear       0.93      0.24      0.38        58\n     gratitude       0.95      0.88      0.91       261\n         grief       0.00      0.00      0.00         6\n           joy       0.58      0.48      0.53       106\n          love       0.67      0.88      0.76       173\n   nervousness       0.00      0.00      0.00         8\n      optimism       0.82      0.28      0.42       119\n         pride       0.00      0.00      0.00         9\n   realization       0.00      0.00      0.00        74\n        relief       0.00      0.00      0.00         8\n       remorse       0.55      0.85      0.67        40\n       sadness       0.37      0.61      0.46        84\n      surprise       0.57      0.25      0.35        95\n       neutral       0.59      0.71      0.64      1592\n\n      accuracy                           0.56      4548\n     macro avg       0.47      0.35      0.35      4548\n  weighted avg       0.56      0.56      0.52      4548\n', 'eval_runtime': 10.4386, 'eval_samples_per_second': 435.69, 'eval_steps_per_second': 13.699, 'epoch': 1.0, 'step': 1135}, {'loss': 1.5393, 'grad_norm': 4.144822597503662, 'learning_rate': 0.0007937062937062938, 'epoch': 2.0, 'step': 2270}, {'eval_loss': 1.4725769758224487, 'eval_accuracy': 0.5657431838170625, 'eval_f1': 0.3837733060525316, 'eval_classification_report': '                precision    recall  f1-score   support\n\n    admiration       0.79      0.58      0.67       326\n     amusement       0.68      0.88      0.77       208\n         anger       0.43      0.55      0.48       109\n     annoyance       0.44      0.14      0.21       164\n      approval       0.70      0.08      0.15       258\n        caring       0.00      0.00      0.00        96\n     confusion       0.38      0.16      0.22       102\n     curiosity       0.42      0.61      0.50       164\n        desire       0.83      0.29      0.43        52\ndisappointment       0.22      0.07      0.10        91\n   disapproval       0.30      0.53      0.38       212\n       disgust       0.45      0.39      0.42        61\n embarrassment       0.75      0.30      0.43        20\n    excitement       0.80      0.08      0.14        52\n          fear       0.67      0.52      0.58        58\n     gratitude       0.97      0.85      0.91       261\n         grief       0.00      0.00      0.00         6\n           joy       0.50      0.58      0.54       106\n          love       0.70      0.87      0.78       173\n   nervousness       0.00      0.00      0.00         8\n      optimism       0.75      0.50      0.60       119\n         pride       0.00      0.00      0.00         9\n   realization       0.26      0.27      0.26        74\n        relief       0.00      0.00      0.00         8\n       remorse       0.61      0.75      0.67        40\n       sadness       0.61      0.40      0.49        84\n      surprise       0.61      0.28      0.39        95\n       neutral       0.56      0.74      0.63      1592\n\n      accuracy                           0.57      4548\n     macro avg       0.48      0.37      0.38      4548\n  weighted avg       0.57      0.57      0.53      4548\n', 'eval_runtime': 9.2676, 'eval_samples_per_second': 490.74, 'eval_steps_per_second': 15.43, 'epoch': 2.0, 'step': 2270}, {'loss': 1.4665, 'grad_norm': 3.35322904586792, 'learning_rate': 0.0006614219114219114, 'epoch': 3.0, 'step': 3405}, {'eval_loss': 1.382096529006958, 'eval_accuracy': 0.5848724714160071, 'eval_f1': 0.37859153487330455, 'eval_classification_report': '                precision    recall  f1-score   support\n\n    admiration       0.62      0.79      0.70       326\n     amusement       0.71      0.79      0.75       208\n         anger       0.31      0.57      0.40       109\n     annoyance       0.31      0.09      0.14       164\n      approval       0.58      0.16      0.24       258\n        caring       0.52      0.12      0.20        96\n     confusion       0.61      0.14      0.22       102\n     curiosity       0.51      0.26      0.35       164\n        desire       0.67      0.58      0.62        52\ndisappointment       0.00      0.00      0.00        91\n   disapproval       0.41      0.11      0.18       212\n       disgust       0.38      0.41      0.39        61\n embarrassment       0.71      0.25      0.37        20\n    excitement       0.30      0.25      0.27        52\n          fear       0.60      0.59      0.59        58\n     gratitude       0.87      0.92      0.90       261\n         grief       0.00      0.00      0.00         6\n           joy       0.66      0.37      0.47       106\n          love       0.65      0.91      0.76       173\n   nervousness       0.00      0.00      0.00         8\n      optimism       0.71      0.47      0.57       119\n         pride       0.00      0.00      0.00         9\n   realization       0.69      0.15      0.24        74\n        relief       0.00      0.00      0.00         8\n       remorse       0.65      0.60      0.62        40\n       sadness       0.60      0.37      0.46        84\n      surprise       0.63      0.39      0.48        95\n       neutral       0.56      0.83      0.67      1592\n\n      accuracy                           0.58      4548\n     macro avg       0.47      0.36      0.38      4548\n  weighted avg       0.56      0.58      0.54      4548\n', 'eval_runtime': 9.5419, 'eval_samples_per_second': 476.635, 'eval_steps_per_second': 14.987, 'epoch': 3.0, 'step': 3405}, {'loss': 1.4129, 'grad_norm': 3.2183728218078613, 'learning_rate': 0.0005291375291375291, 'epoch': 4.0, 'step': 4540}, {'eval_loss': 1.355842113494873, 'eval_accuracy': 0.5960861917326298, 'eval_f1': 0.4331961031469219, 'eval_classification_report': '                precision    recall  f1-score   support\n\n    admiration       0.68      0.76      0.72       326\n     amusement       0.78      0.76      0.77       208\n         anger       0.52      0.40      0.45       109\n     annoyance       0.35      0.18      0.24       164\n      approval       0.51      0.22      0.31       258\n        caring       0.64      0.19      0.29        96\n     confusion       0.61      0.25      0.35       102\n     curiosity       0.40      0.63      0.49       164\n        desire       0.52      0.60      0.55        52\ndisappointment       0.32      0.09      0.14        91\n   disapproval       0.43      0.29      0.35       212\n       disgust       0.31      0.64      0.42        61\n embarrassment       0.62      0.40      0.48        20\n    excitement       0.32      0.31      0.31        52\n          fear       0.64      0.59      0.61        58\n     gratitude       0.95      0.89      0.92       261\n         grief       0.00      0.00      0.00         6\n           joy       0.60      0.52      0.56       106\n          love       0.67      0.90      0.76       173\n   nervousness       0.50      0.12      0.20         8\n      optimism       0.60      0.55      0.58       119\n         pride       1.00      0.22      0.36         9\n   realization       0.80      0.05      0.10        74\n        relief       0.00      0.00      0.00         8\n       remorse       0.57      0.50      0.53        40\n       sadness       0.39      0.63      0.48        84\n      surprise       0.52      0.42      0.47        95\n       neutral       0.61      0.75      0.67      1592\n\n      accuracy                           0.60      4548\n     macro avg       0.53      0.42      0.43      4548\n  weighted avg       0.59      0.60      0.57      4548\n', 'eval_runtime': 17.4997, 'eval_samples_per_second': 259.891, 'eval_steps_per_second': 8.172, 'epoch': 4.0, 'step': 4540}, {'loss': 1.3563, 'grad_norm': 4.044126033782959, 'learning_rate': 0.0003968531468531469, 'epoch': 5.0, 'step': 5675}, {'eval_loss': 1.3744018077850342, 'eval_accuracy': 0.5842128408091469, 'eval_f1': 0.40709260708465295, 'eval_classification_report': '                precision    recall  f1-score   support\n\n    admiration       0.45      0.85      0.59       326\n     amusement       0.70      0.80      0.75       208\n         anger       0.40      0.62      0.49       109\n     annoyance       0.38      0.12      0.18       164\n      approval       0.68      0.15      0.25       258\n        caring       0.56      0.33      0.42        96\n     confusion       0.54      0.27      0.36       102\n     curiosity       0.50      0.35      0.41       164\n        desire       0.74      0.48      0.58        52\ndisappointment       0.25      0.03      0.06        91\n   disapproval       0.53      0.24      0.33       212\n       disgust       0.47      0.38      0.42        61\n embarrassment       0.83      0.25      0.38        20\n    excitement       0.53      0.17      0.26        52\n          fear       0.49      0.64      0.56        58\n     gratitude       0.92      0.91      0.91       261\n         grief       0.00      0.00      0.00         6\n           joy       0.77      0.23      0.35       106\n          love       0.73      0.80      0.76       173\n   nervousness       0.50      0.12      0.20         8\n      optimism       0.65      0.54      0.59       119\n         pride       0.00      0.00      0.00         9\n   realization       0.54      0.18      0.27        74\n        relief       0.00      0.00      0.00         8\n       remorse       0.70      0.47      0.57        40\n       sadness       0.44      0.58      0.50        84\n      surprise       0.52      0.60      0.56        95\n       neutral       0.59      0.76      0.67      1592\n\n      accuracy                           0.58      4548\n     macro avg       0.51      0.39      0.41      4548\n  weighted avg       0.59      0.58      0.55      4548\n', 'eval_runtime': 10.8128, 'eval_samples_per_second': 420.613, 'eval_steps_per_second': 13.225, 'epoch': 5.0, 'step': 5675}, {'loss': 1.2927, 'grad_norm': 3.6080007553100586, 'learning_rate': 0.00026456876456876455, 'epoch': 6.0, 'step': 6810}, {'eval_loss': 1.3141041994094849, 'eval_accuracy': 0.6068601583113457, 'eval_f1': 0.4521312441289594, 'eval_classification_report': '                precision    recall  f1-score   support\n\n    admiration       0.63      0.82      0.71       326\n     amusement       0.66      0.91      0.76       208\n         anger       0.47      0.52      0.49       109\n     annoyance       0.44      0.14      0.21       164\n      approval       0.53      0.24      0.33       258\n        caring       0.49      0.44      0.46        96\n     confusion       0.46      0.31      0.37       102\n     curiosity       0.42      0.74      0.54       164\n        desire       0.56      0.48      0.52        52\ndisappointment       0.25      0.18      0.21        91\n   disapproval       0.63      0.20      0.31       212\n       disgust       0.49      0.48      0.48        61\n embarrassment       0.60      0.45      0.51        20\n    excitement       0.30      0.37      0.33        52\n          fear       0.82      0.48      0.61        58\n     gratitude       0.96      0.89      0.92       261\n         grief       0.00      0.00      0.00         6\n           joy       0.58      0.58      0.58       106\n          love       0.74      0.88      0.81       173\n   nervousness       0.00      0.00      0.00         8\n      optimism       0.66      0.55      0.60       119\n         pride       1.00      0.11      0.20         9\n   realization       0.45      0.26      0.33        74\n        relief       0.00      0.00      0.00         8\n       remorse       0.57      0.65      0.60        40\n       sadness       0.50      0.60      0.54        84\n      surprise       0.53      0.60      0.56        95\n       neutral       0.63      0.71      0.67      1592\n\n      accuracy                           0.61      4548\n     macro avg       0.51      0.45      0.45      4548\n  weighted avg       0.60      0.61      0.59      4548\n', 'eval_runtime': 9.372, 'eval_samples_per_second': 485.277, 'eval_steps_per_second': 15.258, 'epoch': 6.0, 'step': 6810}, {'loss': 1.22, 'grad_norm': 3.122204542160034, 'learning_rate': 0.00013228438228438227, 'epoch': 7.0, 'step': 7945}, {'eval_loss': 1.2712992429733276, 'eval_accuracy': 0.6134564643799473, 'eval_f1': 0.46169189488936624, 'eval_classification_report': '                precision    recall  f1-score   support\n\n    admiration       0.65      0.83      0.73       326\n     amusement       0.72      0.92      0.81       208\n         anger       0.47      0.47      0.47       109\n     annoyance       0.29      0.21      0.24       164\n      approval       0.53      0.27      0.35       258\n        caring       0.60      0.35      0.44        96\n     confusion       0.49      0.32      0.39       102\n     curiosity       0.42      0.38      0.40       164\n        desire       0.63      0.56      0.59        52\ndisappointment       0.33      0.14      0.20        91\n   disapproval       0.51      0.29      0.37       212\n       disgust       0.44      0.51      0.47        61\n embarrassment       0.57      0.40      0.47        20\n    excitement       0.54      0.25      0.34        52\n          fear       0.58      0.71      0.64        58\n     gratitude       0.94      0.90      0.92       261\n         grief       0.00      0.00      0.00         6\n           joy       0.59      0.59      0.59       106\n          love       0.71      0.88      0.78       173\n   nervousness       1.00      0.12      0.22         8\n      optimism       0.61      0.59      0.60       119\n         pride       1.00      0.11      0.20         9\n   realization       0.46      0.16      0.24        74\n        relief       0.00      0.00      0.00         8\n       remorse       0.62      0.70      0.66        40\n       sadness       0.53      0.58      0.55        84\n      surprise       0.47      0.65      0.55        95\n       neutral       0.63      0.74      0.68      1592\n\n      accuracy                           0.61      4548\n     macro avg       0.55      0.45      0.46      4548\n  weighted avg       0.60      0.61      0.59      4548\n', 'eval_runtime': 9.6036, 'eval_samples_per_second': 473.573, 'eval_steps_per_second': 14.89, 'epoch': 7.0, 'step': 7945}, {'loss': 1.1236, 'grad_norm': 3.758175849914551, 'learning_rate': 0.0, 'epoch': 8.0, 'step': 9080}, {'eval_loss': 1.2608574628829956, 'eval_accuracy': 0.6125769569041337, 'eval_f1': 0.46426162585758846, 'eval_classification_report': '                precision    recall  f1-score   support\n\n    admiration       0.69      0.80      0.74       326\n     amusement       0.71      0.89      0.79       208\n         anger       0.44      0.52      0.48       109\n     annoyance       0.34      0.23      0.27       164\n      approval       0.51      0.28      0.36       258\n        caring       0.52      0.45      0.48        96\n     confusion       0.49      0.33      0.40       102\n     curiosity       0.43      0.50      0.46       164\n        desire       0.61      0.52      0.56        52\ndisappointment       0.30      0.18      0.22        91\n   disapproval       0.48      0.33      0.40       212\n       disgust       0.43      0.49      0.46        61\n embarrassment       0.60      0.45      0.51        20\n    excitement       0.47      0.31      0.37        52\n          fear       0.62      0.57      0.59        58\n     gratitude       0.93      0.90      0.91       261\n         grief       0.00      0.00      0.00         6\n           joy       0.61      0.56      0.58       106\n          love       0.71      0.90      0.79       173\n   nervousness       1.00      0.12      0.22         8\n      optimism       0.65      0.61      0.63       119\n         pride       1.00      0.11      0.20         9\n   realization       0.54      0.18      0.27        74\n        relief       0.00      0.00      0.00         8\n       remorse       0.57      0.57      0.57        40\n       sadness       0.44      0.63      0.52        84\n      surprise       0.52      0.54      0.53        95\n       neutral       0.64      0.72      0.68      1592\n\n      accuracy                           0.61      4548\n     macro avg       0.54      0.45      0.46      4548\n  weighted avg       0.60      0.61      0.60      4548\n', 'eval_runtime': 10.0485, 'eval_samples_per_second': 452.606, 'eval_steps_per_second': 14.231, 'epoch': 8.0, 'step': 9080}, {'train_runtime': 932.6827, 'train_samples_per_second': 311.429, 'train_steps_per_second': 9.735, 'total_flos': 5450357510123232.0, 'train_loss': 1.40011603401621, 'epoch': 8.0, 'step': 9080}]
Training Losses ( 8 ): [1.7896, 1.5393, 1.4665, 1.4129, 1.3563, 1.2927, 1.22, 1.1236]
Validation Losses ( 8 ): [1.5113416910171509, 1.4725769758224487, 1.382096529006958, 1.355842113494873, 1.3744018077850342, 1.3141041994094849, 1.2712992429733276, 1.2608574628829956]
Graphs saved to disk
