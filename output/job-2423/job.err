2024-11-27 15:23:20.129644: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-11-27 15:23:20.140462: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1732739000.152397 1734335 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1732739000.156078 1734335 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2024-11-27 15:23:20.171267: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
/usr/local/env_dirs/fall2024/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884
  warnings.warn(
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /opt/models/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Map:   0%|          | 0/4548 [00:00<?, ? examples/s]Map:  22%|██▏       | 1000/4548 [00:00<00:00, 8321.98 examples/s]Map:  66%|██████▌   | 3000/4548 [00:00<00:00, 10740.82 examples/s]Map: 100%|██████████| 4548/4548 [00:00<00:00, 6091.96 examples/s] Map: 100%|██████████| 4548/4548 [00:00<00:00, 6493.61 examples/s]
/usr/local/env_dirs/fall2024/lib/python3.12/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
Traceback (most recent call last):
  File "/home/2021/kturan/courses/COMP551/COMP-551-A4/jobs/../python/BERT_test_script.py", line 139, in <module>
    training_args = TrainingArguments(
                    ^^^^^^^^^^^^^^^^^^
  File "<string>", line 131, in __init__
  File "/usr/local/env_dirs/fall2024/lib/python3.12/site-packages/transformers/training_args.py", line 1575, in __post_init__
    raise ValueError(f"logging strategy {self.logging_strategy} requires non-zero --logging_steps")
ValueError: logging strategy IntervalStrategy.STEPS requires non-zero --logging_steps
