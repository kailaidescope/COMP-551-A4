Starting BERT epoch experiments script
Train method: head+1
Learning rate: 0.001 
Num epochs: 10 
Batch size: 16 
Weight decay: 0.1 
Warmup steps: 500
Loading model
Device: cuda
Name: bert.embeddings.word_embeddings.weight  - Size: torch.Size([30522, 768])  - Requires grad: False
Name: bert.embeddings.position_embeddings.weight  - Size: torch.Size([512, 768])  - Requires grad: False
Name: bert.embeddings.token_type_embeddings.weight  - Size: torch.Size([2, 768])  - Requires grad: False
Name: bert.embeddings.LayerNorm.weight  - Size: torch.Size([768])  - Requires grad: False
Name: bert.embeddings.LayerNorm.bias  - Size: torch.Size([768])  - Requires grad: False
Name: bert.encoder.layer.0.attention.self.query.weight  - Size: torch.Size([768, 768])  - Requires grad: False
Name: bert.encoder.layer.0.attention.self.query.bias  - Size: torch.Size([768])  - Requires grad: False
Name: bert.encoder.layer.0.attention.self.key.weight  - Size: torch.Size([768, 768])  - Requires grad: False
Name: bert.encoder.layer.0.attention.self.key.bias  - Size: torch.Size([768])  - Requires grad: False
Name: bert.encoder.layer.0.attention.self.value.weight  - Size: torch.Size([768, 768])  - Requires grad: False
Name: bert.encoder.layer.0.attention.self.value.bias  - Size: torch.Size([768])  - Requires grad: False
Name: bert.encoder.layer.0.attention.output.dense.weight  - Size: torch.Size([768, 768])  - Requires grad: False
Name: bert.encoder.layer.0.attention.output.dense.bias  - Size: torch.Size([768])  - Requires grad: False
Name: bert.encoder.layer.0.attention.output.LayerNorm.weight  - Size: torch.Size([768])  - Requires grad: False
Name: bert.encoder.layer.0.attention.output.LayerNorm.bias  - Size: torch.Size([768])  - Requires grad: False
Name: bert.encoder.layer.0.intermediate.dense.weight  - Size: torch.Size([3072, 768])  - Requires grad: False
Name: bert.encoder.layer.0.intermediate.dense.bias  - Size: torch.Size([3072])  - Requires grad: False
Name: bert.encoder.layer.0.output.dense.weight  - Size: torch.Size([768, 3072])  - Requires grad: False
Name: bert.encoder.layer.0.output.dense.bias  - Size: torch.Size([768])  - Requires grad: False
Name: bert.encoder.layer.0.output.LayerNorm.weight  - Size: torch.Size([768])  - Requires grad: False
Name: bert.encoder.layer.0.output.LayerNorm.bias  - Size: torch.Size([768])  - Requires grad: False
Name: bert.encoder.layer.1.attention.self.query.weight  - Size: torch.Size([768, 768])  - Requires grad: False
Name: bert.encoder.layer.1.attention.self.query.bias  - Size: torch.Size([768])  - Requires grad: False
Name: bert.encoder.layer.1.attention.self.key.weight  - Size: torch.Size([768, 768])  - Requires grad: False
Name: bert.encoder.layer.1.attention.self.key.bias  - Size: torch.Size([768])  - Requires grad: False
Name: bert.encoder.layer.1.attention.self.value.weight  - Size: torch.Size([768, 768])  - Requires grad: False
Name: bert.encoder.layer.1.attention.self.value.bias  - Size: torch.Size([768])  - Requires grad: False
Name: bert.encoder.layer.1.attention.output.dense.weight  - Size: torch.Size([768, 768])  - Requires grad: False
Name: bert.encoder.layer.1.attention.output.dense.bias  - Size: torch.Size([768])  - Requires grad: False
Name: bert.encoder.layer.1.attention.output.LayerNorm.weight  - Size: torch.Size([768])  - Requires grad: False
Name: bert.encoder.layer.1.attention.output.LayerNorm.bias  - Size: torch.Size([768])  - Requires grad: False
Name: bert.encoder.layer.1.intermediate.dense.weight  - Size: torch.Size([3072, 768])  - Requires grad: False
Name: bert.encoder.layer.1.intermediate.dense.bias  - Size: torch.Size([3072])  - Requires grad: False
Name: bert.encoder.layer.1.output.dense.weight  - Size: torch.Size([768, 3072])  - Requires grad: False
Name: bert.encoder.layer.1.output.dense.bias  - Size: torch.Size([768])  - Requires grad: False
Name: bert.encoder.layer.1.output.LayerNorm.weight  - Size: torch.Size([768])  - Requires grad: False
Name: bert.encoder.layer.1.output.LayerNorm.bias  - Size: torch.Size([768])  - Requires grad: False
Name: bert.encoder.layer.2.attention.self.query.weight  - Size: torch.Size([768, 768])  - Requires grad: False
Name: bert.encoder.layer.2.attention.self.query.bias  - Size: torch.Size([768])  - Requires grad: False
Name: bert.encoder.layer.2.attention.self.key.weight  - Size: torch.Size([768, 768])  - Requires grad: False
Name: bert.encoder.layer.2.attention.self.key.bias  - Size: torch.Size([768])  - Requires grad: False
Name: bert.encoder.layer.2.attention.self.value.weight  - Size: torch.Size([768, 768])  - Requires grad: False
Name: bert.encoder.layer.2.attention.self.value.bias  - Size: torch.Size([768])  - Requires grad: False
Name: bert.encoder.layer.2.attention.output.dense.weight  - Size: torch.Size([768, 768])  - Requires grad: False
Name: bert.encoder.layer.2.attention.output.dense.bias  - Size: torch.Size([768])  - Requires grad: False
Name: bert.encoder.layer.2.attention.output.LayerNorm.weight  - Size: torch.Size([768])  - Requires grad: False
Name: bert.encoder.layer.2.attention.output.LayerNorm.bias  - Size: torch.Size([768])  - Requires grad: False
Name: bert.encoder.layer.2.intermediate.dense.weight  - Size: torch.Size([3072, 768])  - Requires grad: False
Name: bert.encoder.layer.2.intermediate.dense.bias  - Size: torch.Size([3072])  - Requires grad: False
Name: bert.encoder.layer.2.output.dense.weight  - Size: torch.Size([768, 3072])  - Requires grad: False
Name: bert.encoder.layer.2.output.dense.bias  - Size: torch.Size([768])  - Requires grad: False
Name: bert.encoder.layer.2.output.LayerNorm.weight  - Size: torch.Size([768])  - Requires grad: False
Name: bert.encoder.layer.2.output.LayerNorm.bias  - Size: torch.Size([768])  - Requires grad: False
Name: bert.encoder.layer.3.attention.self.query.weight  - Size: torch.Size([768, 768])  - Requires grad: False
Name: bert.encoder.layer.3.attention.self.query.bias  - Size: torch.Size([768])  - Requires grad: False
Name: bert.encoder.layer.3.attention.self.key.weight  - Size: torch.Size([768, 768])  - Requires grad: False
Name: bert.encoder.layer.3.attention.self.key.bias  - Size: torch.Size([768])  - Requires grad: False
Name: bert.encoder.layer.3.attention.self.value.weight  - Size: torch.Size([768, 768])  - Requires grad: False
Name: bert.encoder.layer.3.attention.self.value.bias  - Size: torch.Size([768])  - Requires grad: False
Name: bert.encoder.layer.3.attention.output.dense.weight  - Size: torch.Size([768, 768])  - Requires grad: False
Name: bert.encoder.layer.3.attention.output.dense.bias  - Size: torch.Size([768])  - Requires grad: False
Name: bert.encoder.layer.3.attention.output.LayerNorm.weight  - Size: torch.Size([768])  - Requires grad: False
Name: bert.encoder.layer.3.attention.output.LayerNorm.bias  - Size: torch.Size([768])  - Requires grad: False
Name: bert.encoder.layer.3.intermediate.dense.weight  - Size: torch.Size([3072, 768])  - Requires grad: False
Name: bert.encoder.layer.3.intermediate.dense.bias  - Size: torch.Size([3072])  - Requires grad: False
Name: bert.encoder.layer.3.output.dense.weight  - Size: torch.Size([768, 3072])  - Requires grad: False
Name: bert.encoder.layer.3.output.dense.bias  - Size: torch.Size([768])  - Requires grad: False
Name: bert.encoder.layer.3.output.LayerNorm.weight  - Size: torch.Size([768])  - Requires grad: False
Name: bert.encoder.layer.3.output.LayerNorm.bias  - Size: torch.Size([768])  - Requires grad: False
Name: bert.encoder.layer.4.attention.self.query.weight  - Size: torch.Size([768, 768])  - Requires grad: False
Name: bert.encoder.layer.4.attention.self.query.bias  - Size: torch.Size([768])  - Requires grad: False
Name: bert.encoder.layer.4.attention.self.key.weight  - Size: torch.Size([768, 768])  - Requires grad: False
Name: bert.encoder.layer.4.attention.self.key.bias  - Size: torch.Size([768])  - Requires grad: False
Name: bert.encoder.layer.4.attention.self.value.weight  - Size: torch.Size([768, 768])  - Requires grad: False
Name: bert.encoder.layer.4.attention.self.value.bias  - Size: torch.Size([768])  - Requires grad: False
Name: bert.encoder.layer.4.attention.output.dense.weight  - Size: torch.Size([768, 768])  - Requires grad: False
Name: bert.encoder.layer.4.attention.output.dense.bias  - Size: torch.Size([768])  - Requires grad: False
Name: bert.encoder.layer.4.attention.output.LayerNorm.weight  - Size: torch.Size([768])  - Requires grad: False
Name: bert.encoder.layer.4.attention.output.LayerNorm.bias  - Size: torch.Size([768])  - Requires grad: False
Name: bert.encoder.layer.4.intermediate.dense.weight  - Size: torch.Size([3072, 768])  - Requires grad: False
Name: bert.encoder.layer.4.intermediate.dense.bias  - Size: torch.Size([3072])  - Requires grad: False
Name: bert.encoder.layer.4.output.dense.weight  - Size: torch.Size([768, 3072])  - Requires grad: False
Name: bert.encoder.layer.4.output.dense.bias  - Size: torch.Size([768])  - Requires grad: False
Name: bert.encoder.layer.4.output.LayerNorm.weight  - Size: torch.Size([768])  - Requires grad: False
Name: bert.encoder.layer.4.output.LayerNorm.bias  - Size: torch.Size([768])  - Requires grad: False
Name: bert.encoder.layer.5.attention.self.query.weight  - Size: torch.Size([768, 768])  - Requires grad: False
Name: bert.encoder.layer.5.attention.self.query.bias  - Size: torch.Size([768])  - Requires grad: False
Name: bert.encoder.layer.5.attention.self.key.weight  - Size: torch.Size([768, 768])  - Requires grad: False
Name: bert.encoder.layer.5.attention.self.key.bias  - Size: torch.Size([768])  - Requires grad: False
Name: bert.encoder.layer.5.attention.self.value.weight  - Size: torch.Size([768, 768])  - Requires grad: False
Name: bert.encoder.layer.5.attention.self.value.bias  - Size: torch.Size([768])  - Requires grad: False
Name: bert.encoder.layer.5.attention.output.dense.weight  - Size: torch.Size([768, 768])  - Requires grad: False
Name: bert.encoder.layer.5.attention.output.dense.bias  - Size: torch.Size([768])  - Requires grad: False
Name: bert.encoder.layer.5.attention.output.LayerNorm.weight  - Size: torch.Size([768])  - Requires grad: False
Name: bert.encoder.layer.5.attention.output.LayerNorm.bias  - Size: torch.Size([768])  - Requires grad: False
Name: bert.encoder.layer.5.intermediate.dense.weight  - Size: torch.Size([3072, 768])  - Requires grad: False
Name: bert.encoder.layer.5.intermediate.dense.bias  - Size: torch.Size([3072])  - Requires grad: False
Name: bert.encoder.layer.5.output.dense.weight  - Size: torch.Size([768, 3072])  - Requires grad: False
Name: bert.encoder.layer.5.output.dense.bias  - Size: torch.Size([768])  - Requires grad: False
Name: bert.encoder.layer.5.output.LayerNorm.weight  - Size: torch.Size([768])  - Requires grad: False
Name: bert.encoder.layer.5.output.LayerNorm.bias  - Size: torch.Size([768])  - Requires grad: False
Name: bert.encoder.layer.6.attention.self.query.weight  - Size: torch.Size([768, 768])  - Requires grad: False
Name: bert.encoder.layer.6.attention.self.query.bias  - Size: torch.Size([768])  - Requires grad: False
Name: bert.encoder.layer.6.attention.self.key.weight  - Size: torch.Size([768, 768])  - Requires grad: False
Name: bert.encoder.layer.6.attention.self.key.bias  - Size: torch.Size([768])  - Requires grad: False
Name: bert.encoder.layer.6.attention.self.value.weight  - Size: torch.Size([768, 768])  - Requires grad: False
Name: bert.encoder.layer.6.attention.self.value.bias  - Size: torch.Size([768])  - Requires grad: False
Name: bert.encoder.layer.6.attention.output.dense.weight  - Size: torch.Size([768, 768])  - Requires grad: False
Name: bert.encoder.layer.6.attention.output.dense.bias  - Size: torch.Size([768])  - Requires grad: False
Name: bert.encoder.layer.6.attention.output.LayerNorm.weight  - Size: torch.Size([768])  - Requires grad: False
Name: bert.encoder.layer.6.attention.output.LayerNorm.bias  - Size: torch.Size([768])  - Requires grad: False
Name: bert.encoder.layer.6.intermediate.dense.weight  - Size: torch.Size([3072, 768])  - Requires grad: False
Name: bert.encoder.layer.6.intermediate.dense.bias  - Size: torch.Size([3072])  - Requires grad: False
Name: bert.encoder.layer.6.output.dense.weight  - Size: torch.Size([768, 3072])  - Requires grad: False
Name: bert.encoder.layer.6.output.dense.bias  - Size: torch.Size([768])  - Requires grad: False
Name: bert.encoder.layer.6.output.LayerNorm.weight  - Size: torch.Size([768])  - Requires grad: False
Name: bert.encoder.layer.6.output.LayerNorm.bias  - Size: torch.Size([768])  - Requires grad: False
Name: bert.encoder.layer.7.attention.self.query.weight  - Size: torch.Size([768, 768])  - Requires grad: False
Name: bert.encoder.layer.7.attention.self.query.bias  - Size: torch.Size([768])  - Requires grad: False
Name: bert.encoder.layer.7.attention.self.key.weight  - Size: torch.Size([768, 768])  - Requires grad: False
Name: bert.encoder.layer.7.attention.self.key.bias  - Size: torch.Size([768])  - Requires grad: False
Name: bert.encoder.layer.7.attention.self.value.weight  - Size: torch.Size([768, 768])  - Requires grad: False
Name: bert.encoder.layer.7.attention.self.value.bias  - Size: torch.Size([768])  - Requires grad: False
Name: bert.encoder.layer.7.attention.output.dense.weight  - Size: torch.Size([768, 768])  - Requires grad: False
Name: bert.encoder.layer.7.attention.output.dense.bias  - Size: torch.Size([768])  - Requires grad: False
Name: bert.encoder.layer.7.attention.output.LayerNorm.weight  - Size: torch.Size([768])  - Requires grad: False
Name: bert.encoder.layer.7.attention.output.LayerNorm.bias  - Size: torch.Size([768])  - Requires grad: False
Name: bert.encoder.layer.7.intermediate.dense.weight  - Size: torch.Size([3072, 768])  - Requires grad: False
Name: bert.encoder.layer.7.intermediate.dense.bias  - Size: torch.Size([3072])  - Requires grad: False
Name: bert.encoder.layer.7.output.dense.weight  - Size: torch.Size([768, 3072])  - Requires grad: False
Name: bert.encoder.layer.7.output.dense.bias  - Size: torch.Size([768])  - Requires grad: False
Name: bert.encoder.layer.7.output.LayerNorm.weight  - Size: torch.Size([768])  - Requires grad: False
Name: bert.encoder.layer.7.output.LayerNorm.bias  - Size: torch.Size([768])  - Requires grad: False
Name: bert.encoder.layer.8.attention.self.query.weight  - Size: torch.Size([768, 768])  - Requires grad: False
Name: bert.encoder.layer.8.attention.self.query.bias  - Size: torch.Size([768])  - Requires grad: False
Name: bert.encoder.layer.8.attention.self.key.weight  - Size: torch.Size([768, 768])  - Requires grad: False
Name: bert.encoder.layer.8.attention.self.key.bias  - Size: torch.Size([768])  - Requires grad: False
Name: bert.encoder.layer.8.attention.self.value.weight  - Size: torch.Size([768, 768])  - Requires grad: False
Name: bert.encoder.layer.8.attention.self.value.bias  - Size: torch.Size([768])  - Requires grad: False
Name: bert.encoder.layer.8.attention.output.dense.weight  - Size: torch.Size([768, 768])  - Requires grad: False
Name: bert.encoder.layer.8.attention.output.dense.bias  - Size: torch.Size([768])  - Requires grad: False
Name: bert.encoder.layer.8.attention.output.LayerNorm.weight  - Size: torch.Size([768])  - Requires grad: False
Name: bert.encoder.layer.8.attention.output.LayerNorm.bias  - Size: torch.Size([768])  - Requires grad: False
Name: bert.encoder.layer.8.intermediate.dense.weight  - Size: torch.Size([3072, 768])  - Requires grad: False
Name: bert.encoder.layer.8.intermediate.dense.bias  - Size: torch.Size([3072])  - Requires grad: False
Name: bert.encoder.layer.8.output.dense.weight  - Size: torch.Size([768, 3072])  - Requires grad: False
Name: bert.encoder.layer.8.output.dense.bias  - Size: torch.Size([768])  - Requires grad: False
Name: bert.encoder.layer.8.output.LayerNorm.weight  - Size: torch.Size([768])  - Requires grad: False
Name: bert.encoder.layer.8.output.LayerNorm.bias  - Size: torch.Size([768])  - Requires grad: False
Name: bert.encoder.layer.9.attention.self.query.weight  - Size: torch.Size([768, 768])  - Requires grad: False
Name: bert.encoder.layer.9.attention.self.query.bias  - Size: torch.Size([768])  - Requires grad: False
Name: bert.encoder.layer.9.attention.self.key.weight  - Size: torch.Size([768, 768])  - Requires grad: False
Name: bert.encoder.layer.9.attention.self.key.bias  - Size: torch.Size([768])  - Requires grad: False
Name: bert.encoder.layer.9.attention.self.value.weight  - Size: torch.Size([768, 768])  - Requires grad: False
Name: bert.encoder.layer.9.attention.self.value.bias  - Size: torch.Size([768])  - Requires grad: False
Name: bert.encoder.layer.9.attention.output.dense.weight  - Size: torch.Size([768, 768])  - Requires grad: False
Name: bert.encoder.layer.9.attention.output.dense.bias  - Size: torch.Size([768])  - Requires grad: False
Name: bert.encoder.layer.9.attention.output.LayerNorm.weight  - Size: torch.Size([768])  - Requires grad: False
Name: bert.encoder.layer.9.attention.output.LayerNorm.bias  - Size: torch.Size([768])  - Requires grad: False
Name: bert.encoder.layer.9.intermediate.dense.weight  - Size: torch.Size([3072, 768])  - Requires grad: False
Name: bert.encoder.layer.9.intermediate.dense.bias  - Size: torch.Size([3072])  - Requires grad: False
Name: bert.encoder.layer.9.output.dense.weight  - Size: torch.Size([768, 3072])  - Requires grad: False
Name: bert.encoder.layer.9.output.dense.bias  - Size: torch.Size([768])  - Requires grad: False
Name: bert.encoder.layer.9.output.LayerNorm.weight  - Size: torch.Size([768])  - Requires grad: False
Name: bert.encoder.layer.9.output.LayerNorm.bias  - Size: torch.Size([768])  - Requires grad: False
Name: bert.encoder.layer.10.attention.self.query.weight  - Size: torch.Size([768, 768])  - Requires grad: False
Name: bert.encoder.layer.10.attention.self.query.bias  - Size: torch.Size([768])  - Requires grad: False
Name: bert.encoder.layer.10.attention.self.key.weight  - Size: torch.Size([768, 768])  - Requires grad: False
Name: bert.encoder.layer.10.attention.self.key.bias  - Size: torch.Size([768])  - Requires grad: False
Name: bert.encoder.layer.10.attention.self.value.weight  - Size: torch.Size([768, 768])  - Requires grad: False
Name: bert.encoder.layer.10.attention.self.value.bias  - Size: torch.Size([768])  - Requires grad: False
Name: bert.encoder.layer.10.attention.output.dense.weight  - Size: torch.Size([768, 768])  - Requires grad: False
Name: bert.encoder.layer.10.attention.output.dense.bias  - Size: torch.Size([768])  - Requires grad: False
Name: bert.encoder.layer.10.attention.output.LayerNorm.weight  - Size: torch.Size([768])  - Requires grad: False
Name: bert.encoder.layer.10.attention.output.LayerNorm.bias  - Size: torch.Size([768])  - Requires grad: False
Name: bert.encoder.layer.10.intermediate.dense.weight  - Size: torch.Size([3072, 768])  - Requires grad: False
Name: bert.encoder.layer.10.intermediate.dense.bias  - Size: torch.Size([3072])  - Requires grad: False
Name: bert.encoder.layer.10.output.dense.weight  - Size: torch.Size([768, 3072])  - Requires grad: False
Name: bert.encoder.layer.10.output.dense.bias  - Size: torch.Size([768])  - Requires grad: False
Name: bert.encoder.layer.10.output.LayerNorm.weight  - Size: torch.Size([768])  - Requires grad: False
Name: bert.encoder.layer.10.output.LayerNorm.bias  - Size: torch.Size([768])  - Requires grad: False
Name: bert.encoder.layer.11.attention.self.query.weight  - Size: torch.Size([768, 768])  - Requires grad: True
Name: bert.encoder.layer.11.attention.self.query.bias  - Size: torch.Size([768])  - Requires grad: True
Name: bert.encoder.layer.11.attention.self.key.weight  - Size: torch.Size([768, 768])  - Requires grad: True
Name: bert.encoder.layer.11.attention.self.key.bias  - Size: torch.Size([768])  - Requires grad: True
Name: bert.encoder.layer.11.attention.self.value.weight  - Size: torch.Size([768, 768])  - Requires grad: True
Name: bert.encoder.layer.11.attention.self.value.bias  - Size: torch.Size([768])  - Requires grad: True
Name: bert.encoder.layer.11.attention.output.dense.weight  - Size: torch.Size([768, 768])  - Requires grad: True
Name: bert.encoder.layer.11.attention.output.dense.bias  - Size: torch.Size([768])  - Requires grad: True
Name: bert.encoder.layer.11.attention.output.LayerNorm.weight  - Size: torch.Size([768])  - Requires grad: True
Name: bert.encoder.layer.11.attention.output.LayerNorm.bias  - Size: torch.Size([768])  - Requires grad: True
Name: bert.encoder.layer.11.intermediate.dense.weight  - Size: torch.Size([3072, 768])  - Requires grad: True
Name: bert.encoder.layer.11.intermediate.dense.bias  - Size: torch.Size([3072])  - Requires grad: True
Name: bert.encoder.layer.11.output.dense.weight  - Size: torch.Size([768, 3072])  - Requires grad: True
Name: bert.encoder.layer.11.output.dense.bias  - Size: torch.Size([768])  - Requires grad: True
Name: bert.encoder.layer.11.output.LayerNorm.weight  - Size: torch.Size([768])  - Requires grad: True
Name: bert.encoder.layer.11.output.LayerNorm.bias  - Size: torch.Size([768])  - Requires grad: True
Name: bert.pooler.dense.weight  - Size: torch.Size([768, 768])  - Requires grad: True
Name: bert.pooler.dense.bias  - Size: torch.Size([768])  - Requires grad: True
Name: classifier.weight  - Size: torch.Size([28, 768])  - Requires grad: True
Name: classifier.bias  - Size: torch.Size([28])  - Requires grad: True
Fine-tuning the model on the GoEmotions dataset...
Filtered dataset:
 DatasetDict({
    train: Dataset({
        features: ['text', 'labels', 'id'],
        num_rows: 36308
    })
    validation: Dataset({
        features: ['text', 'labels', 'id'],
        num_rows: 4548
    })
    test: Dataset({
        features: ['text', 'labels', 'id'],
        num_rows: 4590
    })
})
{'loss': 1.8443, 'grad_norm': 7.849698066711426, 'learning_rate': 0.0009202702702702703, 'epoch': 1.0}
{'eval_loss': 1.6183136701583862, 'eval_accuracy': 0.5323218997361477, 'eval_f1': 0.3174277328829003, 'eval_runtime': 3.0543, 'eval_samples_per_second': 1489.069, 'eval_steps_per_second': 93.312, 'epoch': 1.0}
{'loss': 1.5921, 'grad_norm': 4.556777000427246, 'learning_rate': 0.000818018018018018, 'epoch': 2.0}
{'eval_loss': 1.5322531461715698, 'eval_accuracy': 0.5628847845206685, 'eval_f1': 0.37047064632211646, 'eval_runtime': 3.0566, 'eval_samples_per_second': 1487.916, 'eval_steps_per_second': 93.24, 'epoch': 2.0}
{'loss': 1.5206, 'grad_norm': 5.407055854797363, 'learning_rate': 0.0007157657657657658, 'epoch': 3.0}
{'eval_loss': 1.4489761590957642, 'eval_accuracy': 0.5749780123131046, 'eval_f1': 0.3661081860171172, 'eval_runtime': 3.0781, 'eval_samples_per_second': 1477.534, 'eval_steps_per_second': 92.59, 'epoch': 3.0}
{'loss': 1.4584, 'grad_norm': 2.7186031341552734, 'learning_rate': 0.0006135135135135135, 'epoch': 4.0}
{'eval_loss': 1.4369323253631592, 'eval_accuracy': 0.5767370272647317, 'eval_f1': 0.4133081002118913, 'eval_runtime': 3.0914, 'eval_samples_per_second': 1471.178, 'eval_steps_per_second': 92.191, 'epoch': 4.0}
{'loss': 1.4089, 'grad_norm': 6.634760856628418, 'learning_rate': 0.0005112612612612613, 'epoch': 5.0}
{'eval_loss': 1.4418494701385498, 'eval_accuracy': 0.578056288478452, 'eval_f1': 0.3826660360892204, 'eval_runtime': 3.0982, 'eval_samples_per_second': 1467.945, 'eval_steps_per_second': 91.989, 'epoch': 5.0}
{'loss': 1.3638, 'grad_norm': 5.660778045654297, 'learning_rate': 0.000409009009009009, 'epoch': 6.0}
{'eval_loss': 1.376844048500061, 'eval_accuracy': 0.591688654353562, 'eval_f1': 0.41980370926116345, 'eval_runtime': 3.1088, 'eval_samples_per_second': 1462.967, 'eval_steps_per_second': 91.677, 'epoch': 6.0}
{'loss': 1.3093, 'grad_norm': 5.7144694328308105, 'learning_rate': 0.00030675675675675675, 'epoch': 7.0}
{'eval_loss': 1.386403203010559, 'eval_accuracy': 0.5868513632365875, 'eval_f1': 0.43341483412113446, 'eval_runtime': 3.1095, 'eval_samples_per_second': 1462.612, 'eval_steps_per_second': 91.654, 'epoch': 7.0}
{'loss': 1.2603, 'grad_norm': 5.2578206062316895, 'learning_rate': 0.0002045045045045045, 'epoch': 8.0}
{'eval_loss': 1.348254680633545, 'eval_accuracy': 0.5980650835532102, 'eval_f1': 0.44778987584187446, 'eval_runtime': 3.1111, 'eval_samples_per_second': 1461.84, 'eval_steps_per_second': 91.606, 'epoch': 8.0}
{'loss': 1.1941, 'grad_norm': 9.350542068481445, 'learning_rate': 0.00010225225225225224, 'epoch': 9.0}
{'eval_loss': 1.3480846881866455, 'eval_accuracy': 0.5971855760773966, 'eval_f1': 0.45493743004285864, 'eval_runtime': 3.1057, 'eval_samples_per_second': 1464.414, 'eval_steps_per_second': 91.767, 'epoch': 9.0}
{'loss': 1.1453, 'grad_norm': 6.154978275299072, 'learning_rate': 0.0, 'epoch': 10.0}
{'eval_loss': 1.342742681503296, 'eval_accuracy': 0.5993843447669305, 'eval_f1': 0.4604824557106059, 'eval_runtime': 3.1042, 'eval_samples_per_second': 1465.134, 'eval_steps_per_second': 91.812, 'epoch': 10.0}
{'train_runtime': 359.4322, 'train_samples_per_second': 1010.149, 'train_steps_per_second': 63.155, 'train_loss': 1.4097082895959527, 'epoch': 10.0}
Predictions shape:  (4590, 28) 
Labels shape:  (4590, 1) 
Class Predictions:  [24 25 20 ... 27 17 27]
Metrics:
F1: 0.4915723984336146 
Accuracy: 0.6076252723311547
Results: {'f1': [0.3174277328829003, 0.37047064632211646, 0.3661081860171172, 0.4133081002118913, 0.3826660360892204, 0.41980370926116345, 0.43341483412113446, 0.44778987584187446, 0.45493743004285864, 0.4604824557106059, 0.4915723984336146], 'accuracy': [0.5323218997361477, 0.5628847845206685, 0.5749780123131046, 0.5767370272647317, 0.578056288478452, 0.591688654353562, 0.5868513632365875, 0.5980650835532102, 0.5971855760773966, 0.5993843447669305, 0.6076252723311547], 'duration': [38.03528809547424, 73.53875875473022, 109.31206488609314, 145.21668457984924, 181.25546288490295, 217.3602557182312, 253.45783472061157, 289.5620529651642, 325.659850358963, 361.72099566459656, 364.8344089984894], 'final': {'f1': 0.4915723984336146, 'accuracy': 0.6076252723311547, 'duration': 364.83750081062317}}
F1 scores: [0.3174277328829003, 0.37047064632211646, 0.3661081860171172, 0.4133081002118913, 0.3826660360892204, 0.41980370926116345, 0.43341483412113446, 0.44778987584187446, 0.45493743004285864, 0.4604824557106059, 0.4915723984336146]
Accuracies: [0.5323218997361477, 0.5628847845206685, 0.5749780123131046, 0.5767370272647317, 0.578056288478452, 0.591688654353562, 0.5868513632365875, 0.5980650835532102, 0.5971855760773966, 0.5993843447669305, 0.6076252723311547]
Durations: [38.03528809547424, 73.53875875473022, 109.31206488609314, 145.21668457984924, 181.25546288490295, 217.3602557182312, 253.45783472061157, 289.5620529651642, 325.659850358963, 361.72099566459656, 364.8344089984894]
log_history: [{'loss': 1.8443, 'grad_norm': 7.849698066711426, 'learning_rate': 0.0009202702702702703, 'epoch': 1.0, 'step': 2270}, {'eval_loss': 1.6183136701583862, 'eval_accuracy': 0.5323218997361477, 'eval_f1': 0.3174277328829003, 'eval_runtime': 3.0543, 'eval_samples_per_second': 1489.069, 'eval_steps_per_second': 93.312, 'epoch': 1.0, 'step': 2270}, {'loss': 1.5921, 'grad_norm': 4.556777000427246, 'learning_rate': 0.000818018018018018, 'epoch': 2.0, 'step': 4540}, {'eval_loss': 1.5322531461715698, 'eval_accuracy': 0.5628847845206685, 'eval_f1': 0.37047064632211646, 'eval_runtime': 3.0566, 'eval_samples_per_second': 1487.916, 'eval_steps_per_second': 93.24, 'epoch': 2.0, 'step': 4540}, {'loss': 1.5206, 'grad_norm': 5.407055854797363, 'learning_rate': 0.0007157657657657658, 'epoch': 3.0, 'step': 6810}, {'eval_loss': 1.4489761590957642, 'eval_accuracy': 0.5749780123131046, 'eval_f1': 0.3661081860171172, 'eval_runtime': 3.0781, 'eval_samples_per_second': 1477.534, 'eval_steps_per_second': 92.59, 'epoch': 3.0, 'step': 6810}, {'loss': 1.4584, 'grad_norm': 2.7186031341552734, 'learning_rate': 0.0006135135135135135, 'epoch': 4.0, 'step': 9080}, {'eval_loss': 1.4369323253631592, 'eval_accuracy': 0.5767370272647317, 'eval_f1': 0.4133081002118913, 'eval_runtime': 3.0914, 'eval_samples_per_second': 1471.178, 'eval_steps_per_second': 92.191, 'epoch': 4.0, 'step': 9080}, {'loss': 1.4089, 'grad_norm': 6.634760856628418, 'learning_rate': 0.0005112612612612613, 'epoch': 5.0, 'step': 11350}, {'eval_loss': 1.4418494701385498, 'eval_accuracy': 0.578056288478452, 'eval_f1': 0.3826660360892204, 'eval_runtime': 3.0982, 'eval_samples_per_second': 1467.945, 'eval_steps_per_second': 91.989, 'epoch': 5.0, 'step': 11350}, {'loss': 1.3638, 'grad_norm': 5.660778045654297, 'learning_rate': 0.000409009009009009, 'epoch': 6.0, 'step': 13620}, {'eval_loss': 1.376844048500061, 'eval_accuracy': 0.591688654353562, 'eval_f1': 0.41980370926116345, 'eval_runtime': 3.1088, 'eval_samples_per_second': 1462.967, 'eval_steps_per_second': 91.677, 'epoch': 6.0, 'step': 13620}, {'loss': 1.3093, 'grad_norm': 5.7144694328308105, 'learning_rate': 0.00030675675675675675, 'epoch': 7.0, 'step': 15890}, {'eval_loss': 1.386403203010559, 'eval_accuracy': 0.5868513632365875, 'eval_f1': 0.43341483412113446, 'eval_runtime': 3.1095, 'eval_samples_per_second': 1462.612, 'eval_steps_per_second': 91.654, 'epoch': 7.0, 'step': 15890}, {'loss': 1.2603, 'grad_norm': 5.2578206062316895, 'learning_rate': 0.0002045045045045045, 'epoch': 8.0, 'step': 18160}, {'eval_loss': 1.348254680633545, 'eval_accuracy': 0.5980650835532102, 'eval_f1': 0.44778987584187446, 'eval_runtime': 3.1111, 'eval_samples_per_second': 1461.84, 'eval_steps_per_second': 91.606, 'epoch': 8.0, 'step': 18160}, {'loss': 1.1941, 'grad_norm': 9.350542068481445, 'learning_rate': 0.00010225225225225224, 'epoch': 9.0, 'step': 20430}, {'eval_loss': 1.3480846881866455, 'eval_accuracy': 0.5971855760773966, 'eval_f1': 0.45493743004285864, 'eval_runtime': 3.1057, 'eval_samples_per_second': 1464.414, 'eval_steps_per_second': 91.767, 'epoch': 9.0, 'step': 20430}, {'loss': 1.1453, 'grad_norm': 6.154978275299072, 'learning_rate': 0.0, 'epoch': 10.0, 'step': 22700}, {'eval_loss': 1.342742681503296, 'eval_accuracy': 0.5993843447669305, 'eval_f1': 0.4604824557106059, 'eval_runtime': 3.1042, 'eval_samples_per_second': 1465.134, 'eval_steps_per_second': 91.812, 'epoch': 10.0, 'step': 22700}, {'train_runtime': 359.4322, 'train_samples_per_second': 1010.149, 'train_steps_per_second': 63.155, 'total_flos': 6394805400488736.0, 'train_loss': 1.4097082895959527, 'epoch': 10.0, 'step': 22700}]
Training Losses ( 10 ): [1.8443, 1.5921, 1.5206, 1.4584, 1.4089, 1.3638, 1.3093, 1.2603, 1.1941, 1.1453]
Validation Losses ( 10 ): [1.6183136701583862, 1.5322531461715698, 1.4489761590957642, 1.4369323253631592, 1.4418494701385498, 1.376844048500061, 1.386403203010559, 1.348254680633545, 1.3480846881866455, 1.342742681503296]
Graphs saved to disk
