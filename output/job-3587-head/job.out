Starting BERT epoch experiments script
Train method: head
Learning rate: 0.01 
Num epochs: 30 
Batch size: 16 
Weight decay: 0.01
Loading model
Device: cuda
Name: bert.embeddings.word_embeddings.weight  - Size: torch.Size([30522, 768])  - Requires grad: False
Name: bert.embeddings.position_embeddings.weight  - Size: torch.Size([512, 768])  - Requires grad: False
Name: bert.embeddings.token_type_embeddings.weight  - Size: torch.Size([2, 768])  - Requires grad: False
Name: bert.embeddings.LayerNorm.weight  - Size: torch.Size([768])  - Requires grad: False
Name: bert.embeddings.LayerNorm.bias  - Size: torch.Size([768])  - Requires grad: False
Name: bert.encoder.layer.0.attention.self.query.weight  - Size: torch.Size([768, 768])  - Requires grad: False
Name: bert.encoder.layer.0.attention.self.query.bias  - Size: torch.Size([768])  - Requires grad: False
Name: bert.encoder.layer.0.attention.self.key.weight  - Size: torch.Size([768, 768])  - Requires grad: False
Name: bert.encoder.layer.0.attention.self.key.bias  - Size: torch.Size([768])  - Requires grad: False
Name: bert.encoder.layer.0.attention.self.value.weight  - Size: torch.Size([768, 768])  - Requires grad: False
Name: bert.encoder.layer.0.attention.self.value.bias  - Size: torch.Size([768])  - Requires grad: False
Name: bert.encoder.layer.0.attention.output.dense.weight  - Size: torch.Size([768, 768])  - Requires grad: False
Name: bert.encoder.layer.0.attention.output.dense.bias  - Size: torch.Size([768])  - Requires grad: False
Name: bert.encoder.layer.0.attention.output.LayerNorm.weight  - Size: torch.Size([768])  - Requires grad: False
Name: bert.encoder.layer.0.attention.output.LayerNorm.bias  - Size: torch.Size([768])  - Requires grad: False
Name: bert.encoder.layer.0.intermediate.dense.weight  - Size: torch.Size([3072, 768])  - Requires grad: False
Name: bert.encoder.layer.0.intermediate.dense.bias  - Size: torch.Size([3072])  - Requires grad: False
Name: bert.encoder.layer.0.output.dense.weight  - Size: torch.Size([768, 3072])  - Requires grad: False
Name: bert.encoder.layer.0.output.dense.bias  - Size: torch.Size([768])  - Requires grad: False
Name: bert.encoder.layer.0.output.LayerNorm.weight  - Size: torch.Size([768])  - Requires grad: False
Name: bert.encoder.layer.0.output.LayerNorm.bias  - Size: torch.Size([768])  - Requires grad: False
Name: bert.encoder.layer.1.attention.self.query.weight  - Size: torch.Size([768, 768])  - Requires grad: False
Name: bert.encoder.layer.1.attention.self.query.bias  - Size: torch.Size([768])  - Requires grad: False
Name: bert.encoder.layer.1.attention.self.key.weight  - Size: torch.Size([768, 768])  - Requires grad: False
Name: bert.encoder.layer.1.attention.self.key.bias  - Size: torch.Size([768])  - Requires grad: False
Name: bert.encoder.layer.1.attention.self.value.weight  - Size: torch.Size([768, 768])  - Requires grad: False
Name: bert.encoder.layer.1.attention.self.value.bias  - Size: torch.Size([768])  - Requires grad: False
Name: bert.encoder.layer.1.attention.output.dense.weight  - Size: torch.Size([768, 768])  - Requires grad: False
Name: bert.encoder.layer.1.attention.output.dense.bias  - Size: torch.Size([768])  - Requires grad: False
Name: bert.encoder.layer.1.attention.output.LayerNorm.weight  - Size: torch.Size([768])  - Requires grad: False
Name: bert.encoder.layer.1.attention.output.LayerNorm.bias  - Size: torch.Size([768])  - Requires grad: False
Name: bert.encoder.layer.1.intermediate.dense.weight  - Size: torch.Size([3072, 768])  - Requires grad: False
Name: bert.encoder.layer.1.intermediate.dense.bias  - Size: torch.Size([3072])  - Requires grad: False
Name: bert.encoder.layer.1.output.dense.weight  - Size: torch.Size([768, 3072])  - Requires grad: False
Name: bert.encoder.layer.1.output.dense.bias  - Size: torch.Size([768])  - Requires grad: False
Name: bert.encoder.layer.1.output.LayerNorm.weight  - Size: torch.Size([768])  - Requires grad: False
Name: bert.encoder.layer.1.output.LayerNorm.bias  - Size: torch.Size([768])  - Requires grad: False
Name: bert.encoder.layer.2.attention.self.query.weight  - Size: torch.Size([768, 768])  - Requires grad: False
Name: bert.encoder.layer.2.attention.self.query.bias  - Size: torch.Size([768])  - Requires grad: False
Name: bert.encoder.layer.2.attention.self.key.weight  - Size: torch.Size([768, 768])  - Requires grad: False
Name: bert.encoder.layer.2.attention.self.key.bias  - Size: torch.Size([768])  - Requires grad: False
Name: bert.encoder.layer.2.attention.self.value.weight  - Size: torch.Size([768, 768])  - Requires grad: False
Name: bert.encoder.layer.2.attention.self.value.bias  - Size: torch.Size([768])  - Requires grad: False
Name: bert.encoder.layer.2.attention.output.dense.weight  - Size: torch.Size([768, 768])  - Requires grad: False
Name: bert.encoder.layer.2.attention.output.dense.bias  - Size: torch.Size([768])  - Requires grad: False
Name: bert.encoder.layer.2.attention.output.LayerNorm.weight  - Size: torch.Size([768])  - Requires grad: False
Name: bert.encoder.layer.2.attention.output.LayerNorm.bias  - Size: torch.Size([768])  - Requires grad: False
Name: bert.encoder.layer.2.intermediate.dense.weight  - Size: torch.Size([3072, 768])  - Requires grad: False
Name: bert.encoder.layer.2.intermediate.dense.bias  - Size: torch.Size([3072])  - Requires grad: False
Name: bert.encoder.layer.2.output.dense.weight  - Size: torch.Size([768, 3072])  - Requires grad: False
Name: bert.encoder.layer.2.output.dense.bias  - Size: torch.Size([768])  - Requires grad: False
Name: bert.encoder.layer.2.output.LayerNorm.weight  - Size: torch.Size([768])  - Requires grad: False
Name: bert.encoder.layer.2.output.LayerNorm.bias  - Size: torch.Size([768])  - Requires grad: False
Name: bert.encoder.layer.3.attention.self.query.weight  - Size: torch.Size([768, 768])  - Requires grad: False
Name: bert.encoder.layer.3.attention.self.query.bias  - Size: torch.Size([768])  - Requires grad: False
Name: bert.encoder.layer.3.attention.self.key.weight  - Size: torch.Size([768, 768])  - Requires grad: False
Name: bert.encoder.layer.3.attention.self.key.bias  - Size: torch.Size([768])  - Requires grad: False
Name: bert.encoder.layer.3.attention.self.value.weight  - Size: torch.Size([768, 768])  - Requires grad: False
Name: bert.encoder.layer.3.attention.self.value.bias  - Size: torch.Size([768])  - Requires grad: False
Name: bert.encoder.layer.3.attention.output.dense.weight  - Size: torch.Size([768, 768])  - Requires grad: False
Name: bert.encoder.layer.3.attention.output.dense.bias  - Size: torch.Size([768])  - Requires grad: False
Name: bert.encoder.layer.3.attention.output.LayerNorm.weight  - Size: torch.Size([768])  - Requires grad: False
Name: bert.encoder.layer.3.attention.output.LayerNorm.bias  - Size: torch.Size([768])  - Requires grad: False
Name: bert.encoder.layer.3.intermediate.dense.weight  - Size: torch.Size([3072, 768])  - Requires grad: False
Name: bert.encoder.layer.3.intermediate.dense.bias  - Size: torch.Size([3072])  - Requires grad: False
Name: bert.encoder.layer.3.output.dense.weight  - Size: torch.Size([768, 3072])  - Requires grad: False
Name: bert.encoder.layer.3.output.dense.bias  - Size: torch.Size([768])  - Requires grad: False
Name: bert.encoder.layer.3.output.LayerNorm.weight  - Size: torch.Size([768])  - Requires grad: False
Name: bert.encoder.layer.3.output.LayerNorm.bias  - Size: torch.Size([768])  - Requires grad: False
Name: bert.encoder.layer.4.attention.self.query.weight  - Size: torch.Size([768, 768])  - Requires grad: False
Name: bert.encoder.layer.4.attention.self.query.bias  - Size: torch.Size([768])  - Requires grad: False
Name: bert.encoder.layer.4.attention.self.key.weight  - Size: torch.Size([768, 768])  - Requires grad: False
Name: bert.encoder.layer.4.attention.self.key.bias  - Size: torch.Size([768])  - Requires grad: False
Name: bert.encoder.layer.4.attention.self.value.weight  - Size: torch.Size([768, 768])  - Requires grad: False
Name: bert.encoder.layer.4.attention.self.value.bias  - Size: torch.Size([768])  - Requires grad: False
Name: bert.encoder.layer.4.attention.output.dense.weight  - Size: torch.Size([768, 768])  - Requires grad: False
Name: bert.encoder.layer.4.attention.output.dense.bias  - Size: torch.Size([768])  - Requires grad: False
Name: bert.encoder.layer.4.attention.output.LayerNorm.weight  - Size: torch.Size([768])  - Requires grad: False
Name: bert.encoder.layer.4.attention.output.LayerNorm.bias  - Size: torch.Size([768])  - Requires grad: False
Name: bert.encoder.layer.4.intermediate.dense.weight  - Size: torch.Size([3072, 768])  - Requires grad: False
Name: bert.encoder.layer.4.intermediate.dense.bias  - Size: torch.Size([3072])  - Requires grad: False
Name: bert.encoder.layer.4.output.dense.weight  - Size: torch.Size([768, 3072])  - Requires grad: False
Name: bert.encoder.layer.4.output.dense.bias  - Size: torch.Size([768])  - Requires grad: False
Name: bert.encoder.layer.4.output.LayerNorm.weight  - Size: torch.Size([768])  - Requires grad: False
Name: bert.encoder.layer.4.output.LayerNorm.bias  - Size: torch.Size([768])  - Requires grad: False
Name: bert.encoder.layer.5.attention.self.query.weight  - Size: torch.Size([768, 768])  - Requires grad: False
Name: bert.encoder.layer.5.attention.self.query.bias  - Size: torch.Size([768])  - Requires grad: False
Name: bert.encoder.layer.5.attention.self.key.weight  - Size: torch.Size([768, 768])  - Requires grad: False
Name: bert.encoder.layer.5.attention.self.key.bias  - Size: torch.Size([768])  - Requires grad: False
Name: bert.encoder.layer.5.attention.self.value.weight  - Size: torch.Size([768, 768])  - Requires grad: False
Name: bert.encoder.layer.5.attention.self.value.bias  - Size: torch.Size([768])  - Requires grad: False
Name: bert.encoder.layer.5.attention.output.dense.weight  - Size: torch.Size([768, 768])  - Requires grad: False
Name: bert.encoder.layer.5.attention.output.dense.bias  - Size: torch.Size([768])  - Requires grad: False
Name: bert.encoder.layer.5.attention.output.LayerNorm.weight  - Size: torch.Size([768])  - Requires grad: False
Name: bert.encoder.layer.5.attention.output.LayerNorm.bias  - Size: torch.Size([768])  - Requires grad: False
Name: bert.encoder.layer.5.intermediate.dense.weight  - Size: torch.Size([3072, 768])  - Requires grad: False
Name: bert.encoder.layer.5.intermediate.dense.bias  - Size: torch.Size([3072])  - Requires grad: False
Name: bert.encoder.layer.5.output.dense.weight  - Size: torch.Size([768, 3072])  - Requires grad: False
Name: bert.encoder.layer.5.output.dense.bias  - Size: torch.Size([768])  - Requires grad: False
Name: bert.encoder.layer.5.output.LayerNorm.weight  - Size: torch.Size([768])  - Requires grad: False
Name: bert.encoder.layer.5.output.LayerNorm.bias  - Size: torch.Size([768])  - Requires grad: False
Name: bert.encoder.layer.6.attention.self.query.weight  - Size: torch.Size([768, 768])  - Requires grad: False
Name: bert.encoder.layer.6.attention.self.query.bias  - Size: torch.Size([768])  - Requires grad: False
Name: bert.encoder.layer.6.attention.self.key.weight  - Size: torch.Size([768, 768])  - Requires grad: False
Name: bert.encoder.layer.6.attention.self.key.bias  - Size: torch.Size([768])  - Requires grad: False
Name: bert.encoder.layer.6.attention.self.value.weight  - Size: torch.Size([768, 768])  - Requires grad: False
Name: bert.encoder.layer.6.attention.self.value.bias  - Size: torch.Size([768])  - Requires grad: False
Name: bert.encoder.layer.6.attention.output.dense.weight  - Size: torch.Size([768, 768])  - Requires grad: False
Name: bert.encoder.layer.6.attention.output.dense.bias  - Size: torch.Size([768])  - Requires grad: False
Name: bert.encoder.layer.6.attention.output.LayerNorm.weight  - Size: torch.Size([768])  - Requires grad: False
Name: bert.encoder.layer.6.attention.output.LayerNorm.bias  - Size: torch.Size([768])  - Requires grad: False
Name: bert.encoder.layer.6.intermediate.dense.weight  - Size: torch.Size([3072, 768])  - Requires grad: False
Name: bert.encoder.layer.6.intermediate.dense.bias  - Size: torch.Size([3072])  - Requires grad: False
Name: bert.encoder.layer.6.output.dense.weight  - Size: torch.Size([768, 3072])  - Requires grad: False
Name: bert.encoder.layer.6.output.dense.bias  - Size: torch.Size([768])  - Requires grad: False
Name: bert.encoder.layer.6.output.LayerNorm.weight  - Size: torch.Size([768])  - Requires grad: False
Name: bert.encoder.layer.6.output.LayerNorm.bias  - Size: torch.Size([768])  - Requires grad: False
Name: bert.encoder.layer.7.attention.self.query.weight  - Size: torch.Size([768, 768])  - Requires grad: False
Name: bert.encoder.layer.7.attention.self.query.bias  - Size: torch.Size([768])  - Requires grad: False
Name: bert.encoder.layer.7.attention.self.key.weight  - Size: torch.Size([768, 768])  - Requires grad: False
Name: bert.encoder.layer.7.attention.self.key.bias  - Size: torch.Size([768])  - Requires grad: False
Name: bert.encoder.layer.7.attention.self.value.weight  - Size: torch.Size([768, 768])  - Requires grad: False
Name: bert.encoder.layer.7.attention.self.value.bias  - Size: torch.Size([768])  - Requires grad: False
Name: bert.encoder.layer.7.attention.output.dense.weight  - Size: torch.Size([768, 768])  - Requires grad: False
Name: bert.encoder.layer.7.attention.output.dense.bias  - Size: torch.Size([768])  - Requires grad: False
Name: bert.encoder.layer.7.attention.output.LayerNorm.weight  - Size: torch.Size([768])  - Requires grad: False
Name: bert.encoder.layer.7.attention.output.LayerNorm.bias  - Size: torch.Size([768])  - Requires grad: False
Name: bert.encoder.layer.7.intermediate.dense.weight  - Size: torch.Size([3072, 768])  - Requires grad: False
Name: bert.encoder.layer.7.intermediate.dense.bias  - Size: torch.Size([3072])  - Requires grad: False
Name: bert.encoder.layer.7.output.dense.weight  - Size: torch.Size([768, 3072])  - Requires grad: False
Name: bert.encoder.layer.7.output.dense.bias  - Size: torch.Size([768])  - Requires grad: False
Name: bert.encoder.layer.7.output.LayerNorm.weight  - Size: torch.Size([768])  - Requires grad: False
Name: bert.encoder.layer.7.output.LayerNorm.bias  - Size: torch.Size([768])  - Requires grad: False
Name: bert.encoder.layer.8.attention.self.query.weight  - Size: torch.Size([768, 768])  - Requires grad: False
Name: bert.encoder.layer.8.attention.self.query.bias  - Size: torch.Size([768])  - Requires grad: False
Name: bert.encoder.layer.8.attention.self.key.weight  - Size: torch.Size([768, 768])  - Requires grad: False
Name: bert.encoder.layer.8.attention.self.key.bias  - Size: torch.Size([768])  - Requires grad: False
Name: bert.encoder.layer.8.attention.self.value.weight  - Size: torch.Size([768, 768])  - Requires grad: False
Name: bert.encoder.layer.8.attention.self.value.bias  - Size: torch.Size([768])  - Requires grad: False
Name: bert.encoder.layer.8.attention.output.dense.weight  - Size: torch.Size([768, 768])  - Requires grad: False
Name: bert.encoder.layer.8.attention.output.dense.bias  - Size: torch.Size([768])  - Requires grad: False
Name: bert.encoder.layer.8.attention.output.LayerNorm.weight  - Size: torch.Size([768])  - Requires grad: False
Name: bert.encoder.layer.8.attention.output.LayerNorm.bias  - Size: torch.Size([768])  - Requires grad: False
Name: bert.encoder.layer.8.intermediate.dense.weight  - Size: torch.Size([3072, 768])  - Requires grad: False
Name: bert.encoder.layer.8.intermediate.dense.bias  - Size: torch.Size([3072])  - Requires grad: False
Name: bert.encoder.layer.8.output.dense.weight  - Size: torch.Size([768, 3072])  - Requires grad: False
Name: bert.encoder.layer.8.output.dense.bias  - Size: torch.Size([768])  - Requires grad: False
Name: bert.encoder.layer.8.output.LayerNorm.weight  - Size: torch.Size([768])  - Requires grad: False
Name: bert.encoder.layer.8.output.LayerNorm.bias  - Size: torch.Size([768])  - Requires grad: False
Name: bert.encoder.layer.9.attention.self.query.weight  - Size: torch.Size([768, 768])  - Requires grad: False
Name: bert.encoder.layer.9.attention.self.query.bias  - Size: torch.Size([768])  - Requires grad: False
Name: bert.encoder.layer.9.attention.self.key.weight  - Size: torch.Size([768, 768])  - Requires grad: False
Name: bert.encoder.layer.9.attention.self.key.bias  - Size: torch.Size([768])  - Requires grad: False
Name: bert.encoder.layer.9.attention.self.value.weight  - Size: torch.Size([768, 768])  - Requires grad: False
Name: bert.encoder.layer.9.attention.self.value.bias  - Size: torch.Size([768])  - Requires grad: False
Name: bert.encoder.layer.9.attention.output.dense.weight  - Size: torch.Size([768, 768])  - Requires grad: False
Name: bert.encoder.layer.9.attention.output.dense.bias  - Size: torch.Size([768])  - Requires grad: False
Name: bert.encoder.layer.9.attention.output.LayerNorm.weight  - Size: torch.Size([768])  - Requires grad: False
Name: bert.encoder.layer.9.attention.output.LayerNorm.bias  - Size: torch.Size([768])  - Requires grad: False
Name: bert.encoder.layer.9.intermediate.dense.weight  - Size: torch.Size([3072, 768])  - Requires grad: False
Name: bert.encoder.layer.9.intermediate.dense.bias  - Size: torch.Size([3072])  - Requires grad: False
Name: bert.encoder.layer.9.output.dense.weight  - Size: torch.Size([768, 3072])  - Requires grad: False
Name: bert.encoder.layer.9.output.dense.bias  - Size: torch.Size([768])  - Requires grad: False
Name: bert.encoder.layer.9.output.LayerNorm.weight  - Size: torch.Size([768])  - Requires grad: False
Name: bert.encoder.layer.9.output.LayerNorm.bias  - Size: torch.Size([768])  - Requires grad: False
Name: bert.encoder.layer.10.attention.self.query.weight  - Size: torch.Size([768, 768])  - Requires grad: False
Name: bert.encoder.layer.10.attention.self.query.bias  - Size: torch.Size([768])  - Requires grad: False
Name: bert.encoder.layer.10.attention.self.key.weight  - Size: torch.Size([768, 768])  - Requires grad: False
Name: bert.encoder.layer.10.attention.self.key.bias  - Size: torch.Size([768])  - Requires grad: False
Name: bert.encoder.layer.10.attention.self.value.weight  - Size: torch.Size([768, 768])  - Requires grad: False
Name: bert.encoder.layer.10.attention.self.value.bias  - Size: torch.Size([768])  - Requires grad: False
Name: bert.encoder.layer.10.attention.output.dense.weight  - Size: torch.Size([768, 768])  - Requires grad: False
Name: bert.encoder.layer.10.attention.output.dense.bias  - Size: torch.Size([768])  - Requires grad: False
Name: bert.encoder.layer.10.attention.output.LayerNorm.weight  - Size: torch.Size([768])  - Requires grad: False
Name: bert.encoder.layer.10.attention.output.LayerNorm.bias  - Size: torch.Size([768])  - Requires grad: False
Name: bert.encoder.layer.10.intermediate.dense.weight  - Size: torch.Size([3072, 768])  - Requires grad: False
Name: bert.encoder.layer.10.intermediate.dense.bias  - Size: torch.Size([3072])  - Requires grad: False
Name: bert.encoder.layer.10.output.dense.weight  - Size: torch.Size([768, 3072])  - Requires grad: False
Name: bert.encoder.layer.10.output.dense.bias  - Size: torch.Size([768])  - Requires grad: False
Name: bert.encoder.layer.10.output.LayerNorm.weight  - Size: torch.Size([768])  - Requires grad: False
Name: bert.encoder.layer.10.output.LayerNorm.bias  - Size: torch.Size([768])  - Requires grad: False
Name: bert.encoder.layer.11.attention.self.query.weight  - Size: torch.Size([768, 768])  - Requires grad: False
Name: bert.encoder.layer.11.attention.self.query.bias  - Size: torch.Size([768])  - Requires grad: False
Name: bert.encoder.layer.11.attention.self.key.weight  - Size: torch.Size([768, 768])  - Requires grad: False
Name: bert.encoder.layer.11.attention.self.key.bias  - Size: torch.Size([768])  - Requires grad: False
Name: bert.encoder.layer.11.attention.self.value.weight  - Size: torch.Size([768, 768])  - Requires grad: False
Name: bert.encoder.layer.11.attention.self.value.bias  - Size: torch.Size([768])  - Requires grad: False
Name: bert.encoder.layer.11.attention.output.dense.weight  - Size: torch.Size([768, 768])  - Requires grad: False
Name: bert.encoder.layer.11.attention.output.dense.bias  - Size: torch.Size([768])  - Requires grad: False
Name: bert.encoder.layer.11.attention.output.LayerNorm.weight  - Size: torch.Size([768])  - Requires grad: False
Name: bert.encoder.layer.11.attention.output.LayerNorm.bias  - Size: torch.Size([768])  - Requires grad: False
Name: bert.encoder.layer.11.intermediate.dense.weight  - Size: torch.Size([3072, 768])  - Requires grad: False
Name: bert.encoder.layer.11.intermediate.dense.bias  - Size: torch.Size([3072])  - Requires grad: False
Name: bert.encoder.layer.11.output.dense.weight  - Size: torch.Size([768, 3072])  - Requires grad: False
Name: bert.encoder.layer.11.output.dense.bias  - Size: torch.Size([768])  - Requires grad: False
Name: bert.encoder.layer.11.output.LayerNorm.weight  - Size: torch.Size([768])  - Requires grad: False
Name: bert.encoder.layer.11.output.LayerNorm.bias  - Size: torch.Size([768])  - Requires grad: False
Name: bert.pooler.dense.weight  - Size: torch.Size([768, 768])  - Requires grad: False
Name: bert.pooler.dense.bias  - Size: torch.Size([768])  - Requires grad: False
Name: classifier.weight  - Size: torch.Size([28, 768])  - Requires grad: True
Name: classifier.bias  - Size: torch.Size([28])  - Requires grad: True
Fine-tuning the model on the GoEmotions dataset...
Filtered dataset:
 DatasetDict({
    train: Dataset({
        features: ['text', 'labels', 'id'],
        num_rows: 36308
    })
    validation: Dataset({
        features: ['text', 'labels', 'id'],
        num_rows: 4548
    })
    test: Dataset({
        features: ['text', 'labels', 'id'],
        num_rows: 4590
    })
})
{'loss': 3.2112, 'grad_norm': 7.13248348236084, 'learning_rate': 0.009666666666666667, 'epoch': 1.0}
{'eval_loss': 3.0795786380767822, 'eval_accuracy': 0.35048372911169745, 'eval_f1': 0.09128609522329399, 'eval_runtime': 3.1397, 'eval_samples_per_second': 1448.55, 'eval_steps_per_second': 90.773, 'epoch': 1.0}
{'loss': 3.0762, 'grad_norm': 6.38812780380249, 'learning_rate': 0.009333333333333334, 'epoch': 2.0}
{'eval_loss': 3.2865984439849854, 'eval_accuracy': 0.32783641160949867, 'eval_f1': 0.08013562810733356, 'eval_runtime': 3.1447, 'eval_samples_per_second': 1446.246, 'eval_steps_per_second': 90.629, 'epoch': 2.0}
{'loss': 3.0414, 'grad_norm': 12.393224716186523, 'learning_rate': 0.009000000000000001, 'epoch': 3.0}
{'eval_loss': 2.446727752685547, 'eval_accuracy': 0.3999560246262093, 'eval_f1': 0.1188217972498001, 'eval_runtime': 3.1524, 'eval_samples_per_second': 1442.725, 'eval_steps_per_second': 90.408, 'epoch': 3.0}
{'loss': 3.0119, 'grad_norm': 6.625372409820557, 'learning_rate': 0.008666666666666668, 'epoch': 4.0}
{'eval_loss': 2.604315757751465, 'eval_accuracy': 0.3482849604221636, 'eval_f1': 0.16299296641914549, 'eval_runtime': 3.1505, 'eval_samples_per_second': 1443.589, 'eval_steps_per_second': 90.462, 'epoch': 4.0}
{'loss': 2.9756, 'grad_norm': 12.262434005737305, 'learning_rate': 0.008333333333333333, 'epoch': 5.0}
{'eval_loss': 3.1664445400238037, 'eval_accuracy': 0.3106860158311346, 'eval_f1': 0.10118549190612265, 'eval_runtime': 3.1584, 'eval_samples_per_second': 1439.963, 'eval_steps_per_second': 90.235, 'epoch': 5.0}
{'loss': 2.9186, 'grad_norm': 12.442071914672852, 'learning_rate': 0.008, 'epoch': 6.0}
{'eval_loss': 3.1103317737579346, 'eval_accuracy': 0.3605980650835532, 'eval_f1': 0.10690634468433612, 'eval_runtime': 3.1619, 'eval_samples_per_second': 1438.355, 'eval_steps_per_second': 90.134, 'epoch': 6.0}
{'loss': 2.8992, 'grad_norm': 10.980610847473145, 'learning_rate': 0.007666666666666667, 'epoch': 7.0}
{'eval_loss': 3.1589343547821045, 'eval_accuracy': 0.36169744942832016, 'eval_f1': 0.06876389774699372, 'eval_runtime': 3.1544, 'eval_samples_per_second': 1441.802, 'eval_steps_per_second': 90.35, 'epoch': 7.0}
{'loss': 2.8512, 'grad_norm': 8.931303977966309, 'learning_rate': 0.007333333333333333, 'epoch': 8.0}
{'eval_loss': 3.0317206382751465, 'eval_accuracy': 0.3460861917326297, 'eval_f1': 0.08204488110881776, 'eval_runtime': 3.1537, 'eval_samples_per_second': 1442.108, 'eval_steps_per_second': 90.37, 'epoch': 8.0}
{'loss': 2.7964, 'grad_norm': 6.9742255210876465, 'learning_rate': 0.006999999999999999, 'epoch': 9.0}
{'eval_loss': 2.8867762088775635, 'eval_accuracy': 0.37247141600703604, 'eval_f1': 0.11296215724718829, 'eval_runtime': 3.1597, 'eval_samples_per_second': 1439.398, 'eval_steps_per_second': 90.2, 'epoch': 9.0}
{'loss': 2.7433, 'grad_norm': 7.921012878417969, 'learning_rate': 0.006666666666666666, 'epoch': 10.0}
{'eval_loss': 2.594496250152588, 'eval_accuracy': 0.38060686015831136, 'eval_f1': 0.14229372077469224, 'eval_runtime': 3.1738, 'eval_samples_per_second': 1432.989, 'eval_steps_per_second': 89.798, 'epoch': 10.0}
{'loss': 2.7056, 'grad_norm': 9.864072799682617, 'learning_rate': 0.006333333333333333, 'epoch': 11.0}
{'eval_loss': 2.5740082263946533, 'eval_accuracy': 0.2783641160949868, 'eval_f1': 0.13653855859783068, 'eval_runtime': 3.1727, 'eval_samples_per_second': 1433.482, 'eval_steps_per_second': 89.829, 'epoch': 11.0}
{'loss': 2.6619, 'grad_norm': 6.354542255401611, 'learning_rate': 0.006, 'epoch': 12.0}
{'eval_loss': 2.532738447189331, 'eval_accuracy': 0.4129287598944591, 'eval_f1': 0.13364964070948124, 'eval_runtime': 3.1722, 'eval_samples_per_second': 1433.715, 'eval_steps_per_second': 89.844, 'epoch': 12.0}
{'loss': 2.6161, 'grad_norm': 7.819649696350098, 'learning_rate': 0.005666666666666666, 'epoch': 13.0}
{'eval_loss': 2.2588984966278076, 'eval_accuracy': 0.41908531222515394, 'eval_f1': 0.11006605576710086, 'eval_runtime': 3.173, 'eval_samples_per_second': 1433.362, 'eval_steps_per_second': 89.821, 'epoch': 13.0}
{'loss': 2.5859, 'grad_norm': 11.943540573120117, 'learning_rate': 0.005333333333333333, 'epoch': 14.0}
{'eval_loss': 2.4119439125061035, 'eval_accuracy': 0.39841688654353563, 'eval_f1': 0.12797039413873199, 'eval_runtime': 3.1713, 'eval_samples_per_second': 1434.126, 'eval_steps_per_second': 89.869, 'epoch': 14.0}
{'loss': 2.568, 'grad_norm': 5.027554035186768, 'learning_rate': 0.005, 'epoch': 15.0}
{'eval_loss': 2.571380853652954, 'eval_accuracy': 0.38852242744063326, 'eval_f1': 0.07722680922279881, 'eval_runtime': 3.1682, 'eval_samples_per_second': 1435.518, 'eval_steps_per_second': 89.957, 'epoch': 15.0}
{'loss': 2.5125, 'grad_norm': 7.305894374847412, 'learning_rate': 0.004666666666666667, 'epoch': 16.0}
{'eval_loss': 2.528684139251709, 'eval_accuracy': 0.31200527704485487, 'eval_f1': 0.12960896077684733, 'eval_runtime': 3.1685, 'eval_samples_per_second': 1435.396, 'eval_steps_per_second': 89.949, 'epoch': 16.0}
{'loss': 2.4855, 'grad_norm': 8.29623794555664, 'learning_rate': 0.004333333333333334, 'epoch': 17.0}
{'eval_loss': 2.4349400997161865, 'eval_accuracy': 0.4061125769569041, 'eval_f1': 0.1295842416238654, 'eval_runtime': 3.1626, 'eval_samples_per_second': 1438.064, 'eval_steps_per_second': 90.116, 'epoch': 17.0}
{'loss': 2.4398, 'grad_norm': 7.9529290199279785, 'learning_rate': 0.004, 'epoch': 18.0}
{'eval_loss': 2.4056878089904785, 'eval_accuracy': 0.34432717678100266, 'eval_f1': 0.17079891886245108, 'eval_runtime': 3.1587, 'eval_samples_per_second': 1439.815, 'eval_steps_per_second': 90.226, 'epoch': 18.0}
{'loss': 2.4004, 'grad_norm': 8.057766914367676, 'learning_rate': 0.0036666666666666666, 'epoch': 19.0}
{'eval_loss': 2.3255465030670166, 'eval_accuracy': 0.3770888302550572, 'eval_f1': 0.1414324966061933, 'eval_runtime': 3.168, 'eval_samples_per_second': 1435.598, 'eval_steps_per_second': 89.962, 'epoch': 19.0}
{'loss': 2.3639, 'grad_norm': 5.373482704162598, 'learning_rate': 0.003333333333333333, 'epoch': 20.0}
{'eval_loss': 2.1009726524353027, 'eval_accuracy': 0.4289797713280563, 'eval_f1': 0.1567516713927568, 'eval_runtime': 3.1521, 'eval_samples_per_second': 1442.833, 'eval_steps_per_second': 90.415, 'epoch': 20.0}
{'loss': 2.3259, 'grad_norm': 6.893833637237549, 'learning_rate': 0.003, 'epoch': 21.0}
{'eval_loss': 2.1876208782196045, 'eval_accuracy': 0.3828056288478452, 'eval_f1': 0.14380573473268435, 'eval_runtime': 3.168, 'eval_samples_per_second': 1435.623, 'eval_steps_per_second': 89.963, 'epoch': 21.0}
{'loss': 2.2779, 'grad_norm': 6.956663131713867, 'learning_rate': 0.0026666666666666666, 'epoch': 22.0}
{'eval_loss': 2.2912750244140625, 'eval_accuracy': 0.43051890941073, 'eval_f1': 0.14938438877165328, 'eval_runtime': 3.1584, 'eval_samples_per_second': 1439.959, 'eval_steps_per_second': 90.235, 'epoch': 22.0}
{'loss': 2.2521, 'grad_norm': 7.8437323570251465, 'learning_rate': 0.0023333333333333335, 'epoch': 23.0}
{'eval_loss': 2.07216215133667, 'eval_accuracy': 0.44063324538258575, 'eval_f1': 0.16766498169582575, 'eval_runtime': 3.1609, 'eval_samples_per_second': 1438.821, 'eval_steps_per_second': 90.164, 'epoch': 23.0}
{'loss': 2.2128, 'grad_norm': 5.840041160583496, 'learning_rate': 0.002, 'epoch': 24.0}
{'eval_loss': 2.0694773197174072, 'eval_accuracy': 0.42963940193491645, 'eval_f1': 0.10255867698759359, 'eval_runtime': 3.1631, 'eval_samples_per_second': 1437.821, 'eval_steps_per_second': 90.101, 'epoch': 24.0}
{'loss': 2.1805, 'grad_norm': 7.751210689544678, 'learning_rate': 0.0016666666666666666, 'epoch': 25.0}
{'eval_loss': 2.0507521629333496, 'eval_accuracy': 0.4289797713280563, 'eval_f1': 0.12200485687165134, 'eval_runtime': 3.1677, 'eval_samples_per_second': 1435.729, 'eval_steps_per_second': 89.97, 'epoch': 25.0}
{'loss': 2.1444, 'grad_norm': 11.360203742980957, 'learning_rate': 0.0013333333333333333, 'epoch': 26.0}
{'eval_loss': 1.9905050992965698, 'eval_accuracy': 0.43271767810026385, 'eval_f1': 0.16858606829030007, 'eval_runtime': 3.1634, 'eval_samples_per_second': 1437.706, 'eval_steps_per_second': 90.094, 'epoch': 26.0}
{'loss': 2.1137, 'grad_norm': 6.752874851226807, 'learning_rate': 0.001, 'epoch': 27.0}
{'eval_loss': 1.908690094947815, 'eval_accuracy': 0.4520668425681618, 'eval_f1': 0.17255457678967415, 'eval_runtime': 3.1662, 'eval_samples_per_second': 1436.433, 'eval_steps_per_second': 90.014, 'epoch': 27.0}
{'loss': 2.0747, 'grad_norm': 5.297706127166748, 'learning_rate': 0.0006666666666666666, 'epoch': 28.0}
{'eval_loss': 1.8799313306808472, 'eval_accuracy': 0.4520668425681618, 'eval_f1': 0.18176295951578583, 'eval_runtime': 3.1613, 'eval_samples_per_second': 1438.634, 'eval_steps_per_second': 90.152, 'epoch': 28.0}
{'loss': 2.0392, 'grad_norm': 4.415618896484375, 'learning_rate': 0.0003333333333333333, 'epoch': 29.0}
{'eval_loss': 1.8994642496109009, 'eval_accuracy': 0.4558047493403694, 'eval_f1': 0.1673088196688243, 'eval_runtime': 3.1669, 'eval_samples_per_second': 1436.089, 'eval_steps_per_second': 89.992, 'epoch': 29.0}
{'loss': 2.0131, 'grad_norm': 8.2174711227417, 'learning_rate': 0.0, 'epoch': 30.0}
{'eval_loss': 1.8532921075820923, 'eval_accuracy': 0.45822339489885666, 'eval_f1': 0.18316144242317756, 'eval_runtime': 3.1607, 'eval_samples_per_second': 1438.903, 'eval_steps_per_second': 90.169, 'epoch': 30.0}
{'train_runtime': 986.4223, 'train_samples_per_second': 1104.233, 'train_steps_per_second': 69.037, 'train_loss': 2.549953208172724, 'epoch': 30.0}
Predictions shape:  (4590, 28) 
Labels shape:  (4590, 1) 
Class Predictions:  [27 25 20 ... 27  0 27]
Metrics:
F1: 0.1903400049169668 
Accuracy: 0.4636165577342048
Results: {'f1': [0.09128609522329399, 0.08013562810733356, 0.1188217972498001, 0.16299296641914549, 0.10118549190612265, 0.10690634468433612, 0.06876389774699372, 0.08204488110881776, 0.11296215724718829, 0.14229372077469224, 0.13653855859783068, 0.13364964070948124, 0.11006605576710086, 0.12797039413873199, 0.07722680922279881, 0.12960896077684733, 0.1295842416238654, 0.17079891886245108, 0.1414324966061933, 0.1567516713927568, 0.14380573473268435, 0.14938438877165328, 0.16766498169582575, 0.10255867698759359, 0.12200485687165134, 0.16858606829030007, 0.17255457678967415, 0.18176295951578583, 0.1673088196688243, 0.18316144242317756, 0.1903400049169668], 'accuracy': [0.35048372911169745, 0.32783641160949867, 0.3999560246262093, 0.3482849604221636, 0.3106860158311346, 0.3605980650835532, 0.36169744942832016, 0.3460861917326297, 0.37247141600703604, 0.38060686015831136, 0.2783641160949868, 0.4129287598944591, 0.41908531222515394, 0.39841688654353563, 0.38852242744063326, 0.31200527704485487, 0.4061125769569041, 0.34432717678100266, 0.3770888302550572, 0.4289797713280563, 0.3828056288478452, 0.43051890941073, 0.44063324538258575, 0.42963940193491645, 0.4289797713280563, 0.43271767810026385, 0.4520668425681618, 0.4520668425681618, 0.4558047493403694, 0.45822339489885666, 0.4636165577342048], 'duration': [35.00900053977966, 67.50870394706726, 100.00158405303955, 132.9374704360962, 165.82849502563477, 198.8274428844452, 231.6376953125, 264.5321261882782, 297.2700126171112, 329.96158814430237, 362.77412939071655, 395.50846219062805, 428.65626096725464, 461.52370619773865, 494.2137086391449, 527.0792994499207, 559.8232312202454, 592.548680305481, 625.6634185314178, 658.756073474884, 691.8552391529083, 724.9297256469727, 757.7844586372375, 790.5365669727325, 823.6055686473846, 856.5930871963501, 889.5265340805054, 922.5761182308197, 955.6562941074371, 988.7185342311859, 991.8856890201569], 'final': {'f1': 0.1903400049169668, 'accuracy': 0.4636165577342048, 'duration': 991.8890047073364}}
F1 scores: [0.09128609522329399, 0.08013562810733356, 0.1188217972498001, 0.16299296641914549, 0.10118549190612265, 0.10690634468433612, 0.06876389774699372, 0.08204488110881776, 0.11296215724718829, 0.14229372077469224, 0.13653855859783068, 0.13364964070948124, 0.11006605576710086, 0.12797039413873199, 0.07722680922279881, 0.12960896077684733, 0.1295842416238654, 0.17079891886245108, 0.1414324966061933, 0.1567516713927568, 0.14380573473268435, 0.14938438877165328, 0.16766498169582575, 0.10255867698759359, 0.12200485687165134, 0.16858606829030007, 0.17255457678967415, 0.18176295951578583, 0.1673088196688243, 0.18316144242317756, 0.1903400049169668]
Accuracies: [0.35048372911169745, 0.32783641160949867, 0.3999560246262093, 0.3482849604221636, 0.3106860158311346, 0.3605980650835532, 0.36169744942832016, 0.3460861917326297, 0.37247141600703604, 0.38060686015831136, 0.2783641160949868, 0.4129287598944591, 0.41908531222515394, 0.39841688654353563, 0.38852242744063326, 0.31200527704485487, 0.4061125769569041, 0.34432717678100266, 0.3770888302550572, 0.4289797713280563, 0.3828056288478452, 0.43051890941073, 0.44063324538258575, 0.42963940193491645, 0.4289797713280563, 0.43271767810026385, 0.4520668425681618, 0.4520668425681618, 0.4558047493403694, 0.45822339489885666, 0.4636165577342048]
Durations: [35.00900053977966, 67.50870394706726, 100.00158405303955, 132.9374704360962, 165.82849502563477, 198.8274428844452, 231.6376953125, 264.5321261882782, 297.2700126171112, 329.96158814430237, 362.77412939071655, 395.50846219062805, 428.65626096725464, 461.52370619773865, 494.2137086391449, 527.0792994499207, 559.8232312202454, 592.548680305481, 625.6634185314178, 658.756073474884, 691.8552391529083, 724.9297256469727, 757.7844586372375, 790.5365669727325, 823.6055686473846, 856.5930871963501, 889.5265340805054, 922.5761182308197, 955.6562941074371, 988.7185342311859, 991.8856890201569]
log_history: [{'loss': 3.2112, 'grad_norm': 7.13248348236084, 'learning_rate': 0.009666666666666667, 'epoch': 1.0, 'step': 2270}, {'eval_loss': 3.0795786380767822, 'eval_accuracy': 0.35048372911169745, 'eval_f1': 0.09128609522329399, 'eval_runtime': 3.1397, 'eval_samples_per_second': 1448.55, 'eval_steps_per_second': 90.773, 'epoch': 1.0, 'step': 2270}, {'loss': 3.0762, 'grad_norm': 6.38812780380249, 'learning_rate': 0.009333333333333334, 'epoch': 2.0, 'step': 4540}, {'eval_loss': 3.2865984439849854, 'eval_accuracy': 0.32783641160949867, 'eval_f1': 0.08013562810733356, 'eval_runtime': 3.1447, 'eval_samples_per_second': 1446.246, 'eval_steps_per_second': 90.629, 'epoch': 2.0, 'step': 4540}, {'loss': 3.0414, 'grad_norm': 12.393224716186523, 'learning_rate': 0.009000000000000001, 'epoch': 3.0, 'step': 6810}, {'eval_loss': 2.446727752685547, 'eval_accuracy': 0.3999560246262093, 'eval_f1': 0.1188217972498001, 'eval_runtime': 3.1524, 'eval_samples_per_second': 1442.725, 'eval_steps_per_second': 90.408, 'epoch': 3.0, 'step': 6810}, {'loss': 3.0119, 'grad_norm': 6.625372409820557, 'learning_rate': 0.008666666666666668, 'epoch': 4.0, 'step': 9080}, {'eval_loss': 2.604315757751465, 'eval_accuracy': 0.3482849604221636, 'eval_f1': 0.16299296641914549, 'eval_runtime': 3.1505, 'eval_samples_per_second': 1443.589, 'eval_steps_per_second': 90.462, 'epoch': 4.0, 'step': 9080}, {'loss': 2.9756, 'grad_norm': 12.262434005737305, 'learning_rate': 0.008333333333333333, 'epoch': 5.0, 'step': 11350}, {'eval_loss': 3.1664445400238037, 'eval_accuracy': 0.3106860158311346, 'eval_f1': 0.10118549190612265, 'eval_runtime': 3.1584, 'eval_samples_per_second': 1439.963, 'eval_steps_per_second': 90.235, 'epoch': 5.0, 'step': 11350}, {'loss': 2.9186, 'grad_norm': 12.442071914672852, 'learning_rate': 0.008, 'epoch': 6.0, 'step': 13620}, {'eval_loss': 3.1103317737579346, 'eval_accuracy': 0.3605980650835532, 'eval_f1': 0.10690634468433612, 'eval_runtime': 3.1619, 'eval_samples_per_second': 1438.355, 'eval_steps_per_second': 90.134, 'epoch': 6.0, 'step': 13620}, {'loss': 2.8992, 'grad_norm': 10.980610847473145, 'learning_rate': 0.007666666666666667, 'epoch': 7.0, 'step': 15890}, {'eval_loss': 3.1589343547821045, 'eval_accuracy': 0.36169744942832016, 'eval_f1': 0.06876389774699372, 'eval_runtime': 3.1544, 'eval_samples_per_second': 1441.802, 'eval_steps_per_second': 90.35, 'epoch': 7.0, 'step': 15890}, {'loss': 2.8512, 'grad_norm': 8.931303977966309, 'learning_rate': 0.007333333333333333, 'epoch': 8.0, 'step': 18160}, {'eval_loss': 3.0317206382751465, 'eval_accuracy': 0.3460861917326297, 'eval_f1': 0.08204488110881776, 'eval_runtime': 3.1537, 'eval_samples_per_second': 1442.108, 'eval_steps_per_second': 90.37, 'epoch': 8.0, 'step': 18160}, {'loss': 2.7964, 'grad_norm': 6.9742255210876465, 'learning_rate': 0.006999999999999999, 'epoch': 9.0, 'step': 20430}, {'eval_loss': 2.8867762088775635, 'eval_accuracy': 0.37247141600703604, 'eval_f1': 0.11296215724718829, 'eval_runtime': 3.1597, 'eval_samples_per_second': 1439.398, 'eval_steps_per_second': 90.2, 'epoch': 9.0, 'step': 20430}, {'loss': 2.7433, 'grad_norm': 7.921012878417969, 'learning_rate': 0.006666666666666666, 'epoch': 10.0, 'step': 22700}, {'eval_loss': 2.594496250152588, 'eval_accuracy': 0.38060686015831136, 'eval_f1': 0.14229372077469224, 'eval_runtime': 3.1738, 'eval_samples_per_second': 1432.989, 'eval_steps_per_second': 89.798, 'epoch': 10.0, 'step': 22700}, {'loss': 2.7056, 'grad_norm': 9.864072799682617, 'learning_rate': 0.006333333333333333, 'epoch': 11.0, 'step': 24970}, {'eval_loss': 2.5740082263946533, 'eval_accuracy': 0.2783641160949868, 'eval_f1': 0.13653855859783068, 'eval_runtime': 3.1727, 'eval_samples_per_second': 1433.482, 'eval_steps_per_second': 89.829, 'epoch': 11.0, 'step': 24970}, {'loss': 2.6619, 'grad_norm': 6.354542255401611, 'learning_rate': 0.006, 'epoch': 12.0, 'step': 27240}, {'eval_loss': 2.532738447189331, 'eval_accuracy': 0.4129287598944591, 'eval_f1': 0.13364964070948124, 'eval_runtime': 3.1722, 'eval_samples_per_second': 1433.715, 'eval_steps_per_second': 89.844, 'epoch': 12.0, 'step': 27240}, {'loss': 2.6161, 'grad_norm': 7.819649696350098, 'learning_rate': 0.005666666666666666, 'epoch': 13.0, 'step': 29510}, {'eval_loss': 2.2588984966278076, 'eval_accuracy': 0.41908531222515394, 'eval_f1': 0.11006605576710086, 'eval_runtime': 3.173, 'eval_samples_per_second': 1433.362, 'eval_steps_per_second': 89.821, 'epoch': 13.0, 'step': 29510}, {'loss': 2.5859, 'grad_norm': 11.943540573120117, 'learning_rate': 0.005333333333333333, 'epoch': 14.0, 'step': 31780}, {'eval_loss': 2.4119439125061035, 'eval_accuracy': 0.39841688654353563, 'eval_f1': 0.12797039413873199, 'eval_runtime': 3.1713, 'eval_samples_per_second': 1434.126, 'eval_steps_per_second': 89.869, 'epoch': 14.0, 'step': 31780}, {'loss': 2.568, 'grad_norm': 5.027554035186768, 'learning_rate': 0.005, 'epoch': 15.0, 'step': 34050}, {'eval_loss': 2.571380853652954, 'eval_accuracy': 0.38852242744063326, 'eval_f1': 0.07722680922279881, 'eval_runtime': 3.1682, 'eval_samples_per_second': 1435.518, 'eval_steps_per_second': 89.957, 'epoch': 15.0, 'step': 34050}, {'loss': 2.5125, 'grad_norm': 7.305894374847412, 'learning_rate': 0.004666666666666667, 'epoch': 16.0, 'step': 36320}, {'eval_loss': 2.528684139251709, 'eval_accuracy': 0.31200527704485487, 'eval_f1': 0.12960896077684733, 'eval_runtime': 3.1685, 'eval_samples_per_second': 1435.396, 'eval_steps_per_second': 89.949, 'epoch': 16.0, 'step': 36320}, {'loss': 2.4855, 'grad_norm': 8.29623794555664, 'learning_rate': 0.004333333333333334, 'epoch': 17.0, 'step': 38590}, {'eval_loss': 2.4349400997161865, 'eval_accuracy': 0.4061125769569041, 'eval_f1': 0.1295842416238654, 'eval_runtime': 3.1626, 'eval_samples_per_second': 1438.064, 'eval_steps_per_second': 90.116, 'epoch': 17.0, 'step': 38590}, {'loss': 2.4398, 'grad_norm': 7.9529290199279785, 'learning_rate': 0.004, 'epoch': 18.0, 'step': 40860}, {'eval_loss': 2.4056878089904785, 'eval_accuracy': 0.34432717678100266, 'eval_f1': 0.17079891886245108, 'eval_runtime': 3.1587, 'eval_samples_per_second': 1439.815, 'eval_steps_per_second': 90.226, 'epoch': 18.0, 'step': 40860}, {'loss': 2.4004, 'grad_norm': 8.057766914367676, 'learning_rate': 0.0036666666666666666, 'epoch': 19.0, 'step': 43130}, {'eval_loss': 2.3255465030670166, 'eval_accuracy': 0.3770888302550572, 'eval_f1': 0.1414324966061933, 'eval_runtime': 3.168, 'eval_samples_per_second': 1435.598, 'eval_steps_per_second': 89.962, 'epoch': 19.0, 'step': 43130}, {'loss': 2.3639, 'grad_norm': 5.373482704162598, 'learning_rate': 0.003333333333333333, 'epoch': 20.0, 'step': 45400}, {'eval_loss': 2.1009726524353027, 'eval_accuracy': 0.4289797713280563, 'eval_f1': 0.1567516713927568, 'eval_runtime': 3.1521, 'eval_samples_per_second': 1442.833, 'eval_steps_per_second': 90.415, 'epoch': 20.0, 'step': 45400}, {'loss': 2.3259, 'grad_norm': 6.893833637237549, 'learning_rate': 0.003, 'epoch': 21.0, 'step': 47670}, {'eval_loss': 2.1876208782196045, 'eval_accuracy': 0.3828056288478452, 'eval_f1': 0.14380573473268435, 'eval_runtime': 3.168, 'eval_samples_per_second': 1435.623, 'eval_steps_per_second': 89.963, 'epoch': 21.0, 'step': 47670}, {'loss': 2.2779, 'grad_norm': 6.956663131713867, 'learning_rate': 0.0026666666666666666, 'epoch': 22.0, 'step': 49940}, {'eval_loss': 2.2912750244140625, 'eval_accuracy': 0.43051890941073, 'eval_f1': 0.14938438877165328, 'eval_runtime': 3.1584, 'eval_samples_per_second': 1439.959, 'eval_steps_per_second': 90.235, 'epoch': 22.0, 'step': 49940}, {'loss': 2.2521, 'grad_norm': 7.8437323570251465, 'learning_rate': 0.0023333333333333335, 'epoch': 23.0, 'step': 52210}, {'eval_loss': 2.07216215133667, 'eval_accuracy': 0.44063324538258575, 'eval_f1': 0.16766498169582575, 'eval_runtime': 3.1609, 'eval_samples_per_second': 1438.821, 'eval_steps_per_second': 90.164, 'epoch': 23.0, 'step': 52210}, {'loss': 2.2128, 'grad_norm': 5.840041160583496, 'learning_rate': 0.002, 'epoch': 24.0, 'step': 54480}, {'eval_loss': 2.0694773197174072, 'eval_accuracy': 0.42963940193491645, 'eval_f1': 0.10255867698759359, 'eval_runtime': 3.1631, 'eval_samples_per_second': 1437.821, 'eval_steps_per_second': 90.101, 'epoch': 24.0, 'step': 54480}, {'loss': 2.1805, 'grad_norm': 7.751210689544678, 'learning_rate': 0.0016666666666666666, 'epoch': 25.0, 'step': 56750}, {'eval_loss': 2.0507521629333496, 'eval_accuracy': 0.4289797713280563, 'eval_f1': 0.12200485687165134, 'eval_runtime': 3.1677, 'eval_samples_per_second': 1435.729, 'eval_steps_per_second': 89.97, 'epoch': 25.0, 'step': 56750}, {'loss': 2.1444, 'grad_norm': 11.360203742980957, 'learning_rate': 0.0013333333333333333, 'epoch': 26.0, 'step': 59020}, {'eval_loss': 1.9905050992965698, 'eval_accuracy': 0.43271767810026385, 'eval_f1': 0.16858606829030007, 'eval_runtime': 3.1634, 'eval_samples_per_second': 1437.706, 'eval_steps_per_second': 90.094, 'epoch': 26.0, 'step': 59020}, {'loss': 2.1137, 'grad_norm': 6.752874851226807, 'learning_rate': 0.001, 'epoch': 27.0, 'step': 61290}, {'eval_loss': 1.908690094947815, 'eval_accuracy': 0.4520668425681618, 'eval_f1': 0.17255457678967415, 'eval_runtime': 3.1662, 'eval_samples_per_second': 1436.433, 'eval_steps_per_second': 90.014, 'epoch': 27.0, 'step': 61290}, {'loss': 2.0747, 'grad_norm': 5.297706127166748, 'learning_rate': 0.0006666666666666666, 'epoch': 28.0, 'step': 63560}, {'eval_loss': 1.8799313306808472, 'eval_accuracy': 0.4520668425681618, 'eval_f1': 0.18176295951578583, 'eval_runtime': 3.1613, 'eval_samples_per_second': 1438.634, 'eval_steps_per_second': 90.152, 'epoch': 28.0, 'step': 63560}, {'loss': 2.0392, 'grad_norm': 4.415618896484375, 'learning_rate': 0.0003333333333333333, 'epoch': 29.0, 'step': 65830}, {'eval_loss': 1.8994642496109009, 'eval_accuracy': 0.4558047493403694, 'eval_f1': 0.1673088196688243, 'eval_runtime': 3.1669, 'eval_samples_per_second': 1436.089, 'eval_steps_per_second': 89.992, 'epoch': 29.0, 'step': 65830}, {'loss': 2.0131, 'grad_norm': 8.2174711227417, 'learning_rate': 0.0, 'epoch': 30.0, 'step': 68100}, {'eval_loss': 1.8532921075820923, 'eval_accuracy': 0.45822339489885666, 'eval_f1': 0.18316144242317756, 'eval_runtime': 3.1607, 'eval_samples_per_second': 1438.903, 'eval_steps_per_second': 90.169, 'epoch': 30.0, 'step': 68100}, {'train_runtime': 986.4223, 'train_samples_per_second': 1104.233, 'train_steps_per_second': 69.037, 'total_flos': 1.91895398406264e+16, 'train_loss': 2.549953208172724, 'epoch': 30.0, 'step': 68100}]
Training Losses ( 30 ): [3.2112, 3.0762, 3.0414, 3.0119, 2.9756, 2.9186, 2.8992, 2.8512, 2.7964, 2.7433, 2.7056, 2.6619, 2.6161, 2.5859, 2.568, 2.5125, 2.4855, 2.4398, 2.4004, 2.3639, 2.3259, 2.2779, 2.2521, 2.2128, 2.1805, 2.1444, 2.1137, 2.0747, 2.0392, 2.0131]
Validation Losses ( 30 ): [3.0795786380767822, 3.2865984439849854, 2.446727752685547, 2.604315757751465, 3.1664445400238037, 3.1103317737579346, 3.1589343547821045, 3.0317206382751465, 2.8867762088775635, 2.594496250152588, 2.5740082263946533, 2.532738447189331, 2.2588984966278076, 2.4119439125061035, 2.571380853652954, 2.528684139251709, 2.4349400997161865, 2.4056878089904785, 2.3255465030670166, 2.1009726524353027, 2.1876208782196045, 2.2912750244140625, 2.07216215133667, 2.0694773197174072, 2.0507521629333496, 1.9905050992965698, 1.908690094947815, 1.8799313306808472, 1.8994642496109009, 1.8532921075820923]
Graphs saved to disk
